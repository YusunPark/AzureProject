"""
Azure AI Search ë¬¸ì„œ ê´€ë¦¬ ì„œë¹„ìŠ¤
ë¬¸ì„œ ì—…ë¡œë“œ, ì¸ë±ì‹±, ê²€ìƒ‰ ê¸°ëŠ¥ ì œê³µ
"""
import json
import uuid
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import openai
import hashlib
import re
from config import AZURE_SEARCH_CONFIG, AI_CONFIG

# Azure Search íŒ¨í‚¤ì§€ ì¡°ê±´ë¶€ import
try:
    from azure.search.documents import SearchClient
    from azure.search.documents.indexes import SearchIndexClient
    from azure.search.documents.models import VectorizedQuery
    from azure.core.credentials import AzureKeyCredential
    from azure.search.documents.indexes.models import (
        SearchIndex,
        SimpleField,
        SearchableField,
        SearchField,  # ë²¡í„° í•„ë“œìš©
        SearchFieldDataType,
        VectorSearch,
        VectorSearchProfile,
        HnswAlgorithmConfiguration,
        SemanticConfiguration,
        SemanticSearch,
        SemanticPrioritizedFields,
        SemanticField
    )
    AZURE_SEARCH_AVAILABLE = True
except ImportError:
    print("âš ï¸ Azure Search íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²€ìƒ‰ ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")
    AZURE_SEARCH_AVAILABLE = False
    SearchClient = None
    SearchIndexClient = None
    VectorizedQuery = None
    AzureKeyCredential = None

class AzureSearchService:
    def __init__(self):
        self.available = False
        self.search_client = None
        self.index_client = None
        self.openai_client = None
        self.index_name = "company-documents"  # ê¸°ë³¸ ì¸ë±ìŠ¤ëª…
        self._init_search()
        self._init_openai()
    
    def _init_search(self):
        """Azure Search ì´ˆê¸°í™”"""
        try:
            if not AZURE_SEARCH_AVAILABLE:
                print("âš ï¸ Azure Search íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                self.available = False
                return
                
            if AZURE_SEARCH_CONFIG["endpoint"] and AZURE_SEARCH_CONFIG["admin_key"]:
                credential = AzureKeyCredential(AZURE_SEARCH_CONFIG["admin_key"])
                
                self.search_client = SearchClient(
                    endpoint=AZURE_SEARCH_CONFIG["endpoint"],
                    index_name=self.index_name,
                    credential=credential
                )
                
                self.index_client = SearchIndexClient(
                    endpoint=AZURE_SEARCH_CONFIG["endpoint"],
                    credential=credential
                )
                
                self.available = True
                print("âœ… Azure Search ì´ˆê¸°í™” ì„±ê³µ")
            else:
                print("âš ï¸ Azure Search ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤.")
                self.available = False
                
        except Exception as e:
            print(f"âš ï¸ Azure Search ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.available = False
    
    def _init_openai(self):
        """OpenAI ì´ˆê¸°í™” (ë²¡í„° ì„ë² ë”©ìš©)"""
        try:
            if AI_CONFIG["openai_api_key"] and AI_CONFIG["openai_endpoint"]:
                self.openai_client = openai.AzureOpenAI(
                    azure_endpoint=AI_CONFIG["openai_endpoint"],
                    api_key=AI_CONFIG["openai_api_key"],
                    api_version=AI_CONFIG["api_version"]
                )
        except Exception as e:
            print(f"âš ï¸ OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def create_index_if_not_exists(self):
        """ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ìƒì„± (ë²¡í„° í•„ë“œ ë¬¸ì œ í•´ê²° í¬í•¨)"""
        if not self.available or not AZURE_SEARCH_AVAILABLE:
            return False
        
        try:
            from azure.search.documents.indexes.models import (
                SearchIndex, SimpleField, SearchableField, ComplexField,
                SearchFieldDataType, VectorSearch, HnswAlgorithmConfiguration,
                VectorSearchProfile, SemanticConfiguration, SemanticSearch,
                SemanticPrioritizedFields, SemanticField
            )
            
            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ë²¡í„° í•„ë“œ ê²€ì¦
            index_needs_recreation = False
            try:
                existing_index = self.index_client.get_index(self.index_name)
                
                # contentVector í•„ë“œê°€ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
                vector_field_exists = False
                if self.openai_client:  # OpenAIê°€ í™œì„±í™”ëœ ê²½ìš°ë§Œ ë²¡í„° í•„ë“œ í™•ì¸
                    for field in existing_index.fields:
                        if field.name == "contentVector":
                            if hasattr(field, 'vector_search_dimensions') and field.vector_search_dimensions:
                                vector_field_exists = True
                                break
                    
                    if not vector_field_exists:
                        index_needs_recreation = True
                        print(f"âš ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ì— ë²¡í„° í•„ë“œê°€ ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±í•©ë‹ˆë‹¤.")
                
                if not index_needs_recreation:
                    print(f"âœ… Azure Search ì¸ë±ìŠ¤ '{self.index_name}' ì´ë¯¸ ì¡´ì¬í•˜ê³  ì˜¬ë°”ë¥´ê²Œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return True
                else:
                    # ê¸°ì¡´ ì¸ë±ìŠ¤ ì‚­ì œ
                    self.index_client.delete_index(self.index_name)
                    print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì¸ë±ìŠ¤ '{self.index_name}' ì‚­ì œ ì™„ë£Œ")
                    
            except Exception as e:
                print(f"ğŸ“ ì¸ë±ìŠ¤ í™•ì¸ ì¤‘ (ìƒˆ ì¸ë±ìŠ¤ ìƒì„± ì˜ˆì •): {e}")
                # ì¸ë±ìŠ¤ê°€ ì—†ê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒì‹œ ìƒì„± ì§„í–‰
            
            # ë²¡í„° ê²€ìƒ‰ ì„¤ì •
            vector_search = VectorSearch(
                profiles=[
                    VectorSearchProfile(
                        name="myHnswProfile",
                        algorithm_configuration_name="myHnsw"
                    )
                ],
                algorithms=[
                    HnswAlgorithmConfiguration(name="myHnsw")
                ]
            )
            
            # ì‹œë§¨í‹± ê²€ìƒ‰ ì„¤ì •
            semantic_config = SemanticConfiguration(
                name="my-semantic-config",
                prioritized_fields=SemanticPrioritizedFields(
                    title_field=SemanticField(field_name="title"),
                    content_fields=[SemanticField(field_name="content")]
                )
            )
            
            semantic_search = SemanticSearch(configurations=[semantic_config])
            
            # í•„ë“œ ì •ì˜
            fields = [
                SimpleField(name="id", type=SearchFieldDataType.String, key=True),
                SearchableField(name="title", type=SearchFieldDataType.String),
                SearchableField(name="content", type=SearchFieldDataType.String),
                SearchableField(name="filename", type=SearchFieldDataType.String),
                SimpleField(name="file_id", type=SearchFieldDataType.String),
                SimpleField(name="document_type", type=SearchFieldDataType.String, filterable=True),
                SimpleField(name="upload_date", type=SearchFieldDataType.DateTimeOffset, filterable=True),
                SimpleField(name="file_size", type=SearchFieldDataType.Int32),
                SearchableField(name="keywords", type=SearchFieldDataType.String),
                SearchableField(name="summary", type=SearchFieldDataType.String),
                SimpleField(name="blob_url", type=SearchFieldDataType.String),
                # ë²¡í„° í•„ë“œ (ì„ë² ë”©ì´ ê°€ëŠ¥í•œ ê²½ìš°)
                SearchField(
                    name="contentVector",
                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                    searchable=True,  # Azure Search ìš”êµ¬ì‚¬í•­: ë²¡í„° í•„ë“œëŠ” searchable=True í•„ìš”
                    vector_search_dimensions=3072,  # text-embedding-3-largeëŠ” 3072 ì°¨ì›
                    vector_search_profile_name="myHnswProfile"
                ) if self.openai_client else None
            ]
            
            # None í•„ë“œ ì œê±°
            fields = [f for f in fields if f is not None]
            
            # ì¸ë±ìŠ¤ ìƒì„±
            index = SearchIndex(
                name=self.index_name,
                fields=fields,
                vector_search=vector_search if self.openai_client else None,
                semantic_search=semantic_search
            )
            
            self.index_client.create_index(index)
            print(f"âœ… ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ì¸ë±ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def generate_embedding(self, text: str) -> Optional[List[float]]:
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± - í† í° ê¸¸ì´ ì œí•œ ì²˜ë¦¬"""
        if not self.openai_client:
            return None
        
        try:
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (OpenAI ì„ë² ë”© ëª¨ë¸ í† í° ì œí•œ: 8192)
            # ëŒ€ëµ 1 í† í° = 4 ë¬¸ìë¡œ ê³„ì‚°í•˜ì—¬ ì•ˆì „í•˜ê²Œ 30000ìë¡œ ì œí•œ
            if len(text) > 30000:
                text = text[:30000] + "... (ë‚´ìš© ê¸¸ì´ë¡œ ì¸í•´ ì¼ë¶€ ìƒëµë¨)"
                print(f"âš ï¸ í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì„œ {len(text):,}ìë¡œ ì¶•ì†Œí–ˆìŠµë‹ˆë‹¤.")
            
            response = self.openai_client.embeddings.create(
                model=AI_CONFIG.get("embedding_deployment_name", "text-embedding-3-large"),
                input=text
            )
            return response.data[0].embedding
            
        except Exception as e:
            error_msg = str(e)
            if "maximum context length" in error_msg:
                print(f"âš ï¸ í…ìŠ¤íŠ¸ ê¸¸ì´ ì´ˆê³¼ë¡œ ì„ë² ë”©ì„ ê±´ë„ˆëœë‹ˆë‹¤: {len(text):,}ì")
                # ë” ì§§ê²Œ ìë¥´ê³  ì¬ì‹œë„
                short_text = text[:15000]
                try:
                    response = self.openai_client.embeddings.create(
                        model=AI_CONFIG.get("embedding_deployment_name", "text-embedding-3-large"),
                        input=short_text
                    )
                    return response.data[0].embedding
                except:
                    print("âŒ ì§§ì€ í…ìŠ¤íŠ¸ë¡œë„ ì„ë² ë”© ì‹¤íŒ¨")
                    return None
            else:
                print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {error_msg}")
                return None
    
    def extract_text_content(self, file_content: bytes, filename: str) -> str:
        """íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ì²˜ë¦¬
            file_ext = filename.lower().split('.')[-1]
            
            if file_ext in ['txt', 'md', 'py', 'js', 'html', 'css', 'json', 'csv']:
                # í…ìŠ¤íŠ¸ íŒŒì¼ì€ ì§ì ‘ ë””ì½”ë”©
                return file_content.decode('utf-8', errors='ignore')
            
            elif file_ext in ['docx']:
                # Word ë¬¸ì„œ ì²˜ë¦¬ (python-docx ì‚¬ìš©)
                try:
                    from docx import Document
                    import io
                    
                    doc = Document(io.BytesIO(file_content))
                    text_parts = []
                    for paragraph in doc.paragraphs:
                        text_parts.append(paragraph.text)
                    return '\n'.join(text_parts)
                except:
                    return f"Word ë¬¸ì„œ ì²˜ë¦¬ ì˜¤ë¥˜: {filename}"
            
            elif file_ext == 'pdf':
                # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
                try:
                    import io
                    import PyPDF2
                    
                    # PyPDF2ë¡œ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                    pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_content))
                    text_parts = []
                    
                    for page_num, page in enumerate(pdf_reader.pages):
                        try:
                            page_text = page.extract_text()
                            if page_text.strip():
                                text_parts.append(page_text)
                        except Exception as e:
                            print(f"PDF í˜ì´ì§€ {page_num} ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                            continue
                    
                    extracted_text = '\n'.join(text_parts)
                    
                    # ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì ìœ¼ë©´ pdfplumber ì‹œë„
                    if len(extracted_text.strip()) < 100:
                        try:
                            import pdfplumber
                            
                            with pdfplumber.open(io.BytesIO(file_content)) as pdf:
                                plumber_text = []
                                for page in pdf.pages:
                                    try:
                                        page_text = page.extract_text()
                                        if page_text:
                                            plumber_text.append(page_text)
                                    except:
                                        continue
                                
                                if plumber_text:
                                    extracted_text = '\n'.join(plumber_text)
                        except ImportError:
                            pass  # pdfplumberê°€ ì—†ìœ¼ë©´ PyPDF2 ê²°ê³¼ ì‚¬ìš©
                    
                    if extracted_text.strip():
                        return extracted_text
                    else:
                        return f"PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {filename}"
                        
                except Exception as e:
                    return f"PDF ì²˜ë¦¬ ì˜¤ë¥˜ ({filename}): {str(e)}"
            
            else:
                # ê¸°íƒ€ íŒŒì¼ì€ ë°”ì´ë„ˆë¦¬ë¡œ ì²˜ë¦¬
                return f"ë°”ì´ë„ˆë¦¬ íŒŒì¼: {filename} (í¬ê¸°: {len(file_content)} bytes)"
                
        except Exception as e:
            return f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}"
    
    def upload_document_to_search(self, file_content: bytes, filename: str, 
                                 file_id: str, blob_url: str, 
                                 metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ë¬¸ì„œë¥¼ Azure AI Searchì— ì—…ë¡œë“œ
        
        Args:
            file_content: íŒŒì¼ ë‚´ìš©
            filename: íŒŒì¼ëª…
            file_id: íŒŒì¼ ê³ ìœ  ID
            blob_url: Azure Storage blob URL
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            ì—…ë¡œë“œ ê²°ê³¼
        """
        if not self.available:
            return {"success": False, "error": "Azure Search ì‚¬ìš© ë¶ˆê°€"}
        
        try:
            # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            self.create_index_if_not_exists()
            
            # í…ìŠ¤íŠ¸ ì¶”ì¶œ
            content = self.extract_text_content(file_content, filename)
            
            # ìš”ì•½ ìƒì„± (ë‚´ìš©ì´ ê¸´ ê²½ìš°)
            summary = content[:300] + "..." if len(content) > 300 else content
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
            keywords = self.extract_keywords(content)
            
            # ì œëª© ì¶”ì¶œ (íŒŒì¼ëª…ì—ì„œ í™•ì¥ì ì œê±°)
            title = filename.rsplit('.', 1)[0] if '.' in filename else filename
            
            # ì„ë² ë”© ìƒì„±
            content_vector = self.generate_embedding(content) if self.openai_client else None
            
            # ë¬¸ì„œ ID ìƒì„± (ê²€ìƒ‰ìš©)
            search_doc_id = f"doc_{file_id}"
            
            # ê²€ìƒ‰ ë¬¸ì„œ êµ¬ì„±
            search_document = {
                "id": search_doc_id,
                "title": title,
                "content": content,
                "filename": filename,
                "file_id": file_id,
                "document_type": "training",
                "upload_date": datetime.now(timezone.utc).isoformat(),
                "file_size": len(file_content),
                "keywords": keywords,
                "summary": summary,
                "blob_url": blob_url
            }
            
            # ë²¡í„° í•„ë“œ ì¶”ê°€ (ì„ë² ë”©ì´ ìˆëŠ” ê²½ìš°)
            if content_vector:
                search_document["contentVector"] = content_vector
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° í¬í•¨
            if metadata:
                for key, value in metadata.items():
                    if key not in search_document:
                        search_document[f"meta_{key}"] = str(value)
            
            # ë¬¸ì„œ ì—…ë¡œë“œ
            result = self.search_client.upload_documents([search_document])
            
            return {
                "success": True,
                "search_doc_id": search_doc_id,
                "title": title,
                "content_length": len(content),
                "keywords": keywords,
                "has_embedding": content_vector is not None,
                "upload_result": result
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "filename": filename
            }
    
    def extract_keywords(self, content: str) -> str:
        """ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not content:
            return ""
        
        # í•œê¸€, ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ (3ê¸€ì ì´ìƒ)
        words = re.findall(r'[ê°€-í£]{3,}|[a-zA-Z]{3,}', content)
        
        # ë¹ˆë„ ê³„ì‚°
        word_count = {}
        for word in words:
            word_lower = word.lower()
            word_count[word_lower] = word_count.get(word_lower, 0) + 1
        
        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë°˜í™˜
        top_keywords = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:10]
        return ", ".join([word for word, count in top_keywords])
    
    def search_documents(self, query: str, top: int = 10, 
                        document_type: Optional[str] = None,
                        use_semantic: bool = True) -> List[Dict[str, Any]]:
        """
        ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            document_type: ë¬¸ì„œ íƒ€ì… í•„í„°
            use_semantic: ì‹œë§¨í‹± ê²€ìƒ‰ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡
        """
        if not self.available:
            return []
        
        try:
            # ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì„¤ì •
            search_params = {
                "search_text": query,
                "top": top,
                "include_total_count": True
            }
            
            # í•„í„° ì¶”ê°€
            if document_type:
                search_params["filter"] = f"document_type eq '{document_type}'"
            
            # ì‹œë§¨í‹± ê²€ìƒ‰ ì„¤ì •
            if use_semantic:
                search_params["query_type"] = "semantic"
                search_params["semantic_configuration_name"] = "my-semantic-config"
            
            # ë²¡í„° ê²€ìƒ‰ ì¶”ê°€ (ì„ë² ë”©ì´ ê°€ëŠ¥í•œ ê²½ìš°)
            if self.openai_client:
                query_vector = self.generate_embedding(query)
                if query_vector:
                    search_params["vector_queries"] = [
                        VectorizedQuery(
                            vector=query_vector,
                            k_nearest_neighbors=5,
                            fields="contentVector"
                        )
                    ]
            
            # ê²€ìƒ‰ ì‹¤í–‰
            results = self.search_client.search(**search_params)
            
            # ê²°ê³¼ ë³€í™˜
            documents = []
            for result in results:
                doc = {
                    "id": result["id"],
                    "title": result.get("title", "ì œëª© ì—†ìŒ"),
                    "content": result.get("content", ""),
                    "filename": result.get("filename", ""),
                    "file_id": result.get("file_id", ""),
                    "document_type": result.get("document_type", "training"),
                    "upload_date": result.get("upload_date", ""),
                    "keywords": result.get("keywords", ""),
                    "summary": result.get("summary", ""),
                    "blob_url": result.get("blob_url", ""),
                    "search_score": result.get("@search.score", 0),
                    "search_reranker_score": result.get("@search.reranker_score")
                }
                documents.append(doc)
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì´í„° ì œê³µ
            if not documents:
                documents = self._get_dummy_internal_documents(query, top)
            
            return documents
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ì‹œì—ë„ ë”ë¯¸ ë°ì´í„° ì œê³µ
            return self._get_dummy_internal_documents(query, top)
    
    def _get_dummy_internal_documents(self, query: str, top: int) -> List[Dict[str, Any]]:
        """ë”ë¯¸ ë‚´ë¶€ ë¬¸ì„œ ìƒì„± (ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì„ ë•Œ)"""
        import random
        
        dummy_docs = []
        
        # ë” í˜„ì‹¤ì ì¸ ì‚¬ë‚´ ë¬¸ì„œ í…œí”Œë¦¿
        templates = [
            {
                "title": f"{query} ê´€ë ¨ ì‚¬ë‚´ ê°€ì´ë“œë¼ì¸",
                "content": f"{query}ì— ëŒ€í•œ ì‚¬ë‚´ í‘œì¤€ ê°€ì´ë“œë¼ì¸ì…ë‹ˆë‹¤. íšŒì‚¬ì˜ ì •ì±…ê³¼ ì ˆì°¨, ëª¨ë²” ì‚¬ë¡€ë¥¼ ì •ë¦¬í•œ ê³µì‹ ë¬¸ì„œë¡œ, ì§ì›ë“¤ì´ ì—…ë¬´ì—ì„œ ì°¸ê³ í•´ì•¼ í•  í•µì‹¬ ë‚´ìš©ì„ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.",
                "document_type": "ê°€ì´ë“œë¼ì¸",
                "filename": f"{query}_ê°€ì´ë“œë¼ì¸.pdf"
            },
            {
                "title": f"{query} í”„ë¡œì íŠ¸ ì‚¬ë¡€ ì—°êµ¬",
                "content": f"{query}ì™€ ê´€ë ¨ëœ ê³¼ê±° í”„ë¡œì íŠ¸ ìˆ˜í–‰ ì‚¬ë¡€ì…ë‹ˆë‹¤. í”„ë¡œì íŠ¸ ì§„í–‰ ê³¼ì •, ë°œìƒí•œ ì´ìŠˆì™€ í•´ê²°ë°©ë²•, ì„±ê³¼ ì§€í‘œ ë“±ì„ ìƒì„¸íˆ ê¸°ë¡í•œ ë‚´ë¶€ ìë£Œì…ë‹ˆë‹¤.",
                "document_type": "í”„ë¡œì íŠ¸ ì‚¬ë¡€",
                "filename": f"{query}_í”„ë¡œì íŠ¸ì‚¬ë¡€.docx"
            },
            {
                "title": f"{query} ê¸°ìˆ  ë¬¸ì„œ",
                "content": f"{query}ì— ëŒ€í•œ ê¸°ìˆ ì  ìƒì„¸ ì •ë³´ë¥¼ ë‹´ì€ ì‚¬ë‚´ ê¸°ìˆ  ë¬¸ì„œì…ë‹ˆë‹¤. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜, êµ¬í˜„ ë°©ë²•, ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.",
                "document_type": "ê¸°ìˆ  ë¬¸ì„œ",
                "filename": f"{query}_ê¸°ìˆ ë¬¸ì„œ.md"
            }
        ]
        
        for i, template in enumerate(templates[:min(top, 3)]):
            dummy_docs.append({
                "id": f"dummy_internal_{i+1}",
                "title": template["title"],
                "content": template["content"],
                "filename": template["filename"],
                "file_id": f"file_{i+1}",
                "document_type": template["document_type"],
                "upload_date": "2024-01-01",
                "keywords": query,
                "summary": template["content"][:100] + "...",
                "blob_url": f"https://example.blob.core.windows.net/{template['filename']}",
                "search_score": 0.8 - (i * 0.1),
                "search_reranker_score": None
            })
        
        return dummy_docs

    def delete_document(self, search_doc_id: str) -> bool:
        """
        ê²€ìƒ‰ ì¸ë±ìŠ¤ì—ì„œ ë¬¸ì„œ ì‚­ì œ
        
        Args:
            search_doc_id: ê²€ìƒ‰ ë¬¸ì„œ ID
            
        Returns:
            ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        if not self.available:
            return False
        
        try:
            self.search_client.delete_documents([{"id": search_doc_id}])
            return True
        except Exception as e:
            print(f"ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_document_by_file_id(self, file_id: str) -> Optional[Dict[str, Any]]:
        """
        íŒŒì¼ IDë¡œ ë¬¸ì„œ ê²€ìƒ‰
        
        Args:
            file_id: íŒŒì¼ ID
            
        Returns:
            ë¬¸ì„œ ì •ë³´ ë˜ëŠ” None
        """
        if not self.available:
            return None
        
        try:
            results = self.search_client.search(
                search_text="*",
                filter=f"file_id eq '{file_id}'",
                top=1
            )
            
            for result in results:
                return {
                    "id": result["id"],
                    "title": result.get("title", ""),
                    "content": result.get("content", ""),
                    "filename": result.get("filename", ""),
                    "file_id": result.get("file_id", ""),
                    "document_type": result.get("document_type", ""),
                    "upload_date": result.get("upload_date", ""),
                    "keywords": result.get("keywords", ""),
                    "summary": result.get("summary", ""),
                    "blob_url": result.get("blob_url", "")
                }
            
            return None
            
        except Exception as e:
            print(f"ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def list_all_documents(self) -> List[Dict[str, Any]]:
        """
        ëª¨ë“  ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
        
        Returns:
            ì „ì²´ ë¬¸ì„œ ëª©ë¡
        """
        return self.search_documents("*", top=1000, use_semantic=False)
    
    def get_search_statistics(self) -> Dict[str, Any]:
        """
        ê²€ìƒ‰ ì¸ë±ìŠ¤ í†µê³„
        
        Returns:
            í†µê³„ ì •ë³´
        """
        if not self.available:
            return {"available": False}
        
        try:
            # ì „ì²´ ë¬¸ì„œ ìˆ˜ ì¡°íšŒ
            results = self.search_client.search(
                search_text="*",
                top=0,
                include_total_count=True
            )
            
            total_count = results.get_count() if hasattr(results, 'get_count') else 0
            
            return {
                "available": True,
                "index_name": self.index_name,
                "total_documents": total_count,
                "endpoint": AZURE_SEARCH_CONFIG["endpoint"],
                "has_embedding": self.openai_client is not None
            }
            
        except Exception as e:
            return {
                "available": True,
                "error": str(e),
                "index_name": self.index_name
            }