"""
AI ì„œë¹„ìŠ¤ ëª¨ë“ˆ - Azure OpenAIì™€ Tavily ê²€ìƒ‰ì„ ì‚¬ìš©
ê¸°íšìì™€ ë³´ê³ ì„œ ì œì‘ìë¥¼ ìœ„í•œ ì „ë¬¸ ë¶„ì„ ë„êµ¬
"""
import openai
import time
import streamlit as st
import json
import re
from typing import List, Dict, Any, Optional, Tuple
from tavily import TavilyClient
from config import AI_CONFIG, TAVILY_CONFIG

class AIService:
    def __init__(self):
        # Azure OpenAI ì„¤ì •
        try:
            if AI_CONFIG["openai_api_key"] and AI_CONFIG["openai_endpoint"]:
                self.client = openai.AzureOpenAI(
                    azure_endpoint=AI_CONFIG["openai_endpoint"],
                    api_key=AI_CONFIG["openai_api_key"],
                    api_version=AI_CONFIG["api_version"]
                )
                self.ai_available = True
            else:
                self.client = None
                self.ai_available = False
                print("âš ï¸ Azure OpenAI ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
        except Exception as e:
            self.client = None
            self.ai_available = False
            print(f"âš ï¸ Azure OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Tavily ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸
        try:
            if TAVILY_CONFIG["api_key"]:
                self.tavily_client = TavilyClient(api_key=TAVILY_CONFIG["api_key"])
                self.search_available = True
            else:
                self.tavily_client = None
                self.search_available = False
                print("âš ï¸ Tavily API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            self.tavily_client = None
            self.search_available = False
            print(f"âš ï¸ Tavily ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def test_ai_connection(self) -> Dict[str, Any]:
        """AI ì—°ê²° ìƒíƒœ í…ŒìŠ¤íŠ¸"""
        result = {
            "ai_available": self.ai_available,
            "search_available": self.search_available,
            "endpoint": AI_CONFIG.get("openai_endpoint", "ì—†ìŒ"),
            "model": AI_CONFIG.get("deployment_name", "ì—†ìŒ"),
            "api_key_set": bool(AI_CONFIG.get("openai_api_key")),
            "tavily_key_set": bool(TAVILY_CONFIG.get("api_key"))
        }
        
        if self.ai_available:
            try:
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í˜¸ì¶œ
                response = self.client.chat.completions.create(
                    model=AI_CONFIG["deployment_name"],
                    messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”."}],
                    max_tokens=50,
                    temperature=0.1
                )
                result["test_response"] = response.choices[0].message.content
                result["connection_test"] = "ì„±ê³µ"
                print("âœ… OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            except Exception as e:
                result["connection_test"] = f"ì‹¤íŒ¨: {str(e)}"
                result["test_response"] = None
                print(f"âŒ OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        else:
            result["connection_test"] = "AI ì‚¬ìš© ë¶ˆê°€ëŠ¥"
            result["test_response"] = None
            
        return result
    
    def get_smart_analysis_prompt(self, text: str, user_type: str = "planner") -> str:
        """ì‚¬ìš©ì ìœ í˜•ì— ë§ëŠ” ìŠ¤ë§ˆíŠ¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompts = {
            "planner": """ë‹¹ì‹ ì€ ê²½í—˜ì´ í’ë¶€í•œ ê¸°íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ê¸°íšì ê´€ì ì—ì„œ ë‹¤ìŒì„ ì œê³µí•˜ì„¸ìš”:
            
            1. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (3-5ê°œ ë¶ˆë¦¿í¬ì¸íŠ¸)
            2. ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ (ìš°ì„ ìˆœìœ„ë³„)
            3. ì ì¬ì  ìœ„í—˜ìš”ì†Œì™€ ëŒ€ì•ˆ
            4. ì´í•´ê´€ê³„ìë³„ ê³ ë ¤ì‚¬í•­
            5. ì„±ê³µ ì§€í‘œ ì œì•ˆ
            
            ê²°ê³¼ëŠ” ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”.""",
            
            "reporter": """ë‹¹ì‹ ì€ ì „ë¬¸ ë³´ê³ ì„œ ì‘ì„±ìì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ê³ ì„œ ì‘ì„±ì ê´€ì ì—ì„œ ë‹¤ìŒì„ ì œê³µí•˜ì„¸ìš”:
            
            1. í•µì‹¬ ë©”ì‹œì§€ ìš”ì•½ (í•œ ë¬¸ë‹¨)
            2. ì£¼ìš” ë°ì´í„° í¬ì¸íŠ¸ì™€ í†µê³„
            3. ë…¼ë¦¬ì  êµ¬ì¡° ì œì•ˆ (ì„œë¡ -ë³¸ë¡ -ê²°ë¡ )
            4. ì‹œê°í™” ì œì•ˆ (ì°¨íŠ¸, ê·¸ë˜í”„, í‘œ)
            5. ì¶”ê°€ í•„ìš” ì •ë³´ ëª©ë¡
            
            ê²°ê³¼ëŠ” ì„¤ë“ë ¥ ìˆê³  ë…¼ë¦¬ì ì¸ ë³´ê³ ì„œ í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”.""",
            
            "general": """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì œê³µí•˜ì„¸ìš”:
            
            1. ì£¼ìš” ì£¼ì œì™€ í‚¤ì›Œë“œ
            2. í•µì‹¬ ë‚´ìš© ìš”ì•½
            3. ë…¼ë¦¬ì  íë¦„ ë¶„ì„
            4. ê°œì„  ì œì•ˆ
            5. ê´€ë ¨ ìë£Œ ê²€ìƒ‰ í‚¤ì›Œë“œ
            
            ê²°ê³¼ëŠ” ëª…í™•í•˜ê³  í™œìš©í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì œê³µí•˜ì„¸ìš”."""
        }
        
        return prompts.get(user_type, prompts["general"])
    
    def analyze_text(self, text: str, user_type: str = "general") -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë¶„ì„ - ì‚¬ìš©ì ìœ í˜•ë³„ ë§ì¶¤ ë¶„ì„"""
        if not text.strip():
            return {"keywords": [], "topic": "", "context": "", "summary": "", "analysis": {}}
        
        # AI ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬ ë° ë¡œê·¸
        print(f"ğŸ” AI ë¶„ì„ ìš”ì²­: user_type={user_type}, text_length={len(text)}")
        print(f"ğŸ”§ AI ì‚¬ìš© ê°€ëŠ¥: {self.ai_available}")
        
        if not self.ai_available:
            print("âš ï¸ AI ì‚¬ìš© ë¶ˆê°€ëŠ¥ - ë”ë¯¸ ë°ì´í„° ë°˜í™˜")
            return self._get_dummy_analysis(text, user_type)
        
        try:
            # ì‚¬ìš©ì ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            system_prompt = self.get_smart_analysis_prompt(text, user_type)
            
            print("ğŸš€ Azure OpenAI API í˜¸ì¶œ ì‹œì‘...")
            print(f"ğŸ“ Endpoint: {AI_CONFIG['openai_endpoint']}")
            print(f"ğŸ¤– Model: {AI_CONFIG['deployment_name']}")
            
            response = self.client.chat.completions.create(
                model=AI_CONFIG["deployment_name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{text}"}
                ],
                max_tokens=AI_CONFIG["max_tokens"],
                temperature=AI_CONFIG["temperature"]
            )
            
            print("âœ… Azure OpenAI API í˜¸ì¶œ ì„±ê³µ!")
            print(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response.choices[0].message.content)} ë¬¸ì")
            
            content = response.choices[0].message.content
            
            # êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ìƒì„±
            analysis_result = self._parse_analysis_response(content, user_type)
            
            return {
                "keywords": self._extract_keywords(text),
                "topic": self._extract_topic(content),
                "summary": self._extract_summary(content),
                "analysis": analysis_result,
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text
            }
            
        except Exception as e:
            print(f"âŒ Azure OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            st.error(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._get_dummy_analysis(text, user_type)
    
    def _parse_analysis_response(self, content: str, user_type: str) -> Dict[str, Any]:
        """AI ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ë¡œ íŒŒì‹±"""
        lines = content.split('\n')
        sections = {}
        current_section = ""
        current_content = []
        
        # ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ë¶„ë¥˜
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ë²ˆí˜¸ë‚˜ ì œëª©ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ ê°ì§€
            if re.match(r'^\d+\.', line) or line.startswith('#') or line.endswith(':'):
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content)
                current_section = line
                current_content = []
            else:
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content)
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ êµ¬ì¡°í™”
        if user_type == "planner":
            return {
                "insights": sections.get("1. í•µì‹¬ ì¸ì‚¬ì´íŠ¸", ""),
                "action_items": sections.get("2. ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ", ""),
                "risks": sections.get("3. ì ì¬ì  ìœ„í—˜ìš”ì†Œì™€ ëŒ€ì•ˆ", ""),
                "stakeholders": sections.get("4. ì´í•´ê´€ê³„ìë³„ ê³ ë ¤ì‚¬í•­", ""),
                "metrics": sections.get("5. ì„±ê³µ ì§€í‘œ ì œì•ˆ", ""),
                "raw_content": content
            }
        elif user_type == "reporter":
            return {
                "key_message": sections.get("1. í•µì‹¬ ë©”ì‹œì§€ ìš”ì•½", ""),
                "data_points": sections.get("2. ì£¼ìš” ë°ì´í„° í¬ì¸íŠ¸ì™€ í†µê³„", ""),
                "structure": sections.get("3. ë…¼ë¦¬ì  êµ¬ì¡° ì œì•ˆ", ""),
                "visualization": sections.get("4. ì‹œê°í™” ì œì•ˆ", ""),
                "additional_info": sections.get("5. ì¶”ê°€ í•„ìš” ì •ë³´ ëª©ë¡", ""),
                "raw_content": content
            }
        else:
            return {
                "topics_keywords": sections.get("1. ì£¼ìš” ì£¼ì œì™€ í‚¤ì›Œë“œ", ""),
                "summary": sections.get("2. í•µì‹¬ ë‚´ìš© ìš”ì•½", ""),
                "logic_flow": sections.get("3. ë…¼ë¦¬ì  íë¦„ ë¶„ì„", ""),
                "improvements": sections.get("4. ê°œì„  ì œì•ˆ", ""),
                "search_keywords": sections.get("5. ê´€ë ¨ ìë£Œ ê²€ìƒ‰ í‚¤ì›Œë“œ", ""),
                "raw_content": content
            }
    
    def _get_dummy_analysis(self, text: str, user_type: str) -> Dict[str, Any]:
        """ë”ë¯¸ ë¶„ì„ ê²°ê³¼ (AIê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•Œ)"""
        keywords = self._extract_keywords(text)
        
        if user_type == "planner":
            return {
                "keywords": keywords,
                "topic": "ê¸°íš ë¶„ì„ í•„ìš”",
                "summary": "ê¸°íšì ê´€ì ì—ì„œ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•œ ë‚´ìš©ì…ë‹ˆë‹¤.",
                "analysis": {
                    "insights": "â€¢ ì£¼ìš” ë…¼ì ë“¤ì„ ëª…í™•íˆ ì •ì˜ í•„ìš”\nâ€¢ ì´í•´ê´€ê³„ì ë¶„ì„ ìš”êµ¬\nâ€¢ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ í•„ìš”",
                    "action_items": "1. í˜„í™© ë¶„ì„ ìˆ˜í–‰\n2. ëª©í‘œ ì„¤ì •\n3. ì‹¤í–‰ ê³„íš ì‘ì„±",
                    "risks": "â€¢ ë¶ˆí™•ì‹¤í•œ ìš”ì†Œë“¤ ì¡´ì¬\nâ€¢ ë¦¬ì†ŒìŠ¤ ì œì•½ ê³ ë ¤ í•„ìš”",
                    "stakeholders": "â€¢ ë‚´ë¶€ íŒ€ì›ë“¤\nâ€¢ ê²½ì˜ì§„\nâ€¢ ì™¸ë¶€ íŒŒíŠ¸ë„ˆ",
                    "metrics": "â€¢ ì§„í–‰ë¥  ì¸¡ì •\nâ€¢ í’ˆì§ˆ ì§€í‘œ\nâ€¢ ì„±ê³¼ ì¸¡ì •",
                    "raw_content": f"[ë”ë¯¸ ëª¨ë“œ] {text[:200]}..."
                },
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text
            }
        elif user_type == "reporter":
            return {
                "keywords": keywords,
                "topic": "ë³´ê³ ì„œ ì‘ì„± ì§€ì›",
                "summary": "ë³´ê³ ì„œ ì‘ì„±ì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "analysis": {
                    "key_message": "í•µì‹¬ ë©”ì‹œì§€ë¥¼ ëª…í™•íˆ ì •ì˜í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì „ê°œí•´ì•¼ í•©ë‹ˆë‹¤.",
                    "data_points": "â€¢ ì •ëŸ‰ì  ë°ì´í„° ìˆ˜ì§‘ í•„ìš”\nâ€¢ ë¹„êµ ë¶„ì„ ë°ì´í„°\nâ€¢ íŠ¸ë Œë“œ ë¶„ì„",
                    "structure": "1. ì„œë¡ : ë°°ê²½ê³¼ ëª©ì \n2. ë³¸ë¡ : í˜„í™© ë¶„ì„ê³¼ í•´ê²°ë°©ì•ˆ\n3. ê²°ë¡ : ê¶Œê³ ì‚¬í•­",
                    "visualization": "â€¢ ë§‰ëŒ€ ì°¨íŠ¸: ë¹„êµ ë¶„ì„\nâ€¢ ì„  ê·¸ë˜í”„: íŠ¸ë Œë“œ\nâ€¢ íŒŒì´ ì°¨íŠ¸: ë¹„ìœ¨",
                    "additional_info": "â€¢ ì—…ê³„ ë²¤ì¹˜ë§ˆí¬\nâ€¢ ê²½ìŸì‚¬ ë¶„ì„\nâ€¢ ì‹œì¥ ë™í–¥",
                    "raw_content": f"[ë”ë¯¸ ëª¨ë“œ] {text[:200]}..."
                },
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text
            }
        else:
            return {
                "keywords": keywords,
                "topic": "í…ìŠ¤íŠ¸ ë¶„ì„",
                "summary": "í…ìŠ¤íŠ¸ ë‚´ìš©ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "analysis": {
                    "topics_keywords": f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords[:5])}",
                    "summary": "ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  êµ¬ì¡°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    "logic_flow": "ë…¼ë¦¬ì  íë¦„ì„ ê°œì„ í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "improvements": "â€¢ ëª…í™•í•œ ì œëª© êµ¬ì„±\nâ€¢ ë‹¨ë½ë³„ ì£¼ì œ ëª…í™•í™”\nâ€¢ ê²°ë¡  ê°•í™”",
                    "search_keywords": f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords[:3])}",
                    "raw_content": f"[ë”ë¯¸ ëª¨ë“œ] {text[:200]}..."
                },
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text
            }
    
    def search_related_documents(self, query: str, keywords: List[str] = None, user_type: str = "general") -> List[Dict[str, Any]]:
        """Tavilyë¥¼ ì‚¬ìš©í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ - ì‚¬ìš©ì íƒ€ì…ë³„ ë§ì¶¤ ê²€ìƒ‰"""
        if not self.search_available:
            return self._get_intelligent_dummy_results(query, keywords, user_type)
        
        try:
            # ì‚¬ìš©ì íƒ€ì…ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
            search_query = self._optimize_search_query(query, keywords, user_type)
            
            # Tavily ê²€ìƒ‰ ì‹¤í–‰
            response = self.tavily_client.search(
                query=search_query,
                search_depth=TAVILY_CONFIG["search_depth"],
                max_results=TAVILY_CONFIG["max_results"],
                include_domains=self._get_relevant_domains(user_type)
            )
            
            # ê²°ê³¼ ë³€í™˜ ë° í•„í„°ë§
            documents = []
            for i, result in enumerate(response.get("results", [])):
                doc = {
                    "id": i + 1,
                    "title": result.get("title", "ì œëª© ì—†ìŒ"),
                    "summary": self._generate_smart_summary(result.get("content", ""), user_type),
                    "content": result.get("content", ""),
                    "source": result.get("url", ""),
                    "relevance_score": self._calculate_relevance_score(result, query, keywords, user_type),
                    "keywords": keywords[:5] if keywords else [],
                    "user_type": user_type,
                    "document_type": self._classify_document_type(result, user_type)
                }
                documents.append(doc)
            
            # ê´€ë ¨ë„ìˆœ ì •ë ¬ ë° í•„í„°ë§
            documents.sort(key=lambda x: x["relevance_score"], reverse=True)
            return documents[:6]  # ìµœëŒ€ 6ê°œ
            
        except Exception as e:
            st.warning(f"ì‹¤ì‹œê°„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} - ë¡œì»¬ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._get_intelligent_dummy_results(query, keywords, user_type)
    
    def _optimize_search_query(self, query: str, keywords: List[str], user_type: str) -> str:
        """ì‚¬ìš©ì íƒ€ì…ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”"""
        base_query = query
        
        # í‚¤ì›Œë“œ ì¶”ê°€
        if keywords:
            base_query += " " + " ".join(keywords[:3])
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ ê²€ìƒ‰ì–´ ì¶”ê°€
        type_keywords = {
            "planner": ["ê¸°íš", "ì „ëµ", "ê³„íš", "ë°©ì•ˆ", "strategy", "planning"],
            "reporter": ["ë³´ê³ ì„œ", "ë¶„ì„", "ë°ì´í„°", "í†µê³„", "report", "analysis"],
            "general": ["ì •ë³´", "ìë£Œ", "ê°€ì´ë“œ", "ë°©ë²•", "information"]
        }
        
        if user_type in type_keywords:
            base_query += " " + " OR ".join(type_keywords[user_type][:2])
        
        return base_query
    
    def _get_relevant_domains(self, user_type: str) -> List[str]:
        """ì‚¬ìš©ì íƒ€ì…ë³„ ê´€ë ¨ ë„ë©”ì¸ ë°˜í™˜"""
        domains = {
            "planner": ["harvard.edu", "mit.edu", "mckinsey.com", "pwc.com", "deloitte.com"],
            "reporter": ["statista.com", "bloomberg.com", "reuters.com", "economist.com"],
            "general": []  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš©
        }
        return domains.get(user_type, [])
    
    def _generate_smart_summary(self, content: str, user_type: str) -> str:
        """ì‚¬ìš©ì íƒ€ì…ë³„ ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ìƒì„±"""
        if not content or len(content) < 100:
            return content[:200] + "..." if len(content) > 200 else content
        
        # ê°„ë‹¨í•œ ìš”ì•½ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ AI ìš”ì•½ ì‚¬ìš© ê°€ëŠ¥)
        sentences = content.split('. ')
        
        if user_type == "planner":
            # ê¸°íšìë¥¼ ìœ„í•œ ì•¡ì…˜ ì¤‘ì‹¬ ìš”ì•½
            key_sentences = [s for s in sentences if any(word in s.lower() 
                           for word in ['ê³„íš', 'ì „ëµ', 'ë°©ë²•', 'ë°©ì•ˆ', 'ëª©í‘œ', 'strategy', 'plan'])]
        elif user_type == "reporter":
            # ë³´ê³ ì„œ ì‘ì„±ìë¥¼ ìœ„í•œ ë°ì´í„° ì¤‘ì‹¬ ìš”ì•½
            key_sentences = [s for s in sentences if any(word in s.lower() 
                           for word in ['ë°ì´í„°', 'í†µê³„', 'ê²°ê³¼', 'ë¶„ì„', 'ìˆ˜ì¹˜', 'data', 'analysis'])]
        else:
            # ì¼ë°˜ì ì¸ ìš”ì•½
            key_sentences = sentences[:3]
        
        if not key_sentences:
            key_sentences = sentences[:2]
        
        summary = '. '.join(key_sentences[:2])
        return summary[:300] + "..." if len(summary) > 300 else summary
    
    def _calculate_relevance_score(self, result: Dict, query: str, keywords: List[str], user_type: str) -> float:
        """ì‚¬ìš©ì íƒ€ì…ë³„ ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°"""
        score = result.get("score", 0.5)
        
        title = result.get("title", "").lower()
        content = result.get("content", "").lower()
        query_lower = query.lower()
        
        # ê¸°ë³¸ ì ìˆ˜
        relevance = score
        
        # ì œëª© ë§¤ì¹­ ë³´ë„ˆìŠ¤
        if query_lower in title:
            relevance += 0.3
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        if keywords:
            keyword_matches = sum(1 for kw in keywords if kw.lower() in title or kw.lower() in content)
            relevance += (keyword_matches / len(keywords)) * 0.2
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ ë³´ë„ˆìŠ¤
        type_bonus_keywords = {
            "planner": ["ê¸°íš", "ì „ëµ", "ê³„íš", "ë°©ì•ˆ", "strategy", "planning", "roadmap"],
            "reporter": ["ë³´ê³ ì„œ", "ë¶„ì„", "ë°ì´í„°", "í†µê³„", "report", "analysis", "study"],
            "general": []
        }
        
        if user_type in type_bonus_keywords:
            bonus_matches = sum(1 for kw in type_bonus_keywords[user_type] 
                              if kw in title or kw in content)
            relevance += bonus_matches * 0.1
        
        return min(1.0, relevance)
    
    def _classify_document_type(self, result: Dict, user_type: str) -> str:
        """ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜"""
        title = result.get("title", "").lower()
        url = result.get("url", "").lower()
        
        if any(word in title for word in ["ë³´ê³ ì„œ", "report", "ë¶„ì„", "analysis"]):
            return "ë³´ê³ ì„œ"
        elif any(word in title for word in ["ê°€ì´ë“œ", "guide", "ë°©ë²•", "how-to"]):
            return "ê°€ì´ë“œ"
        elif any(word in url for word in ["pdf", "doc", "paper"]):
            return "ë¬¸ì„œ"
        elif any(word in title for word in ["ë‰´ìŠ¤", "news", "ê¸°ì‚¬"]):
            return "ë‰´ìŠ¤"
        else:
            return "ì¼ë°˜"
    
    def _get_intelligent_dummy_results(self, query: str, keywords: List[str], user_type: str) -> List[Dict[str, Any]]:
        """ì§€ëŠ¥í˜• ë”ë¯¸ ê²€ìƒ‰ ê²°ê³¼ - ì‚¬ìš©ì íƒ€ì…ë³„ ë§ì¶¤í˜•"""
        try:
            # ë¡œì»¬ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
            with open('data/sample_documents.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            documents = data.get("documents", [])
            
            # ì‚¬ìš©ì íƒ€ì…ë³„ í•„í„°ë§ ë° ì ìˆ˜ ì¡°ì •
            filtered_docs = []
            for doc in documents:
                relevance = self._calculate_local_relevance(doc, query, keywords, user_type)
                if relevance > 0.3:  # ìµœì†Œ ê´€ë ¨ë„ ì„ê³„ê°’
                    doc["relevance_score"] = relevance
                    doc["user_type"] = user_type
                    doc["document_type"] = self._classify_local_document(doc, user_type)
                    doc["summary"] = self._generate_smart_summary(doc.get("content", ""), user_type)
                    filtered_docs.append(doc)
            
            # ê´€ë ¨ë„ìˆœ ì •ë ¬
            filtered_docs.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            # ì‚¬ìš©ì íƒ€ì…ë³„ ì¶”ê°€ ë”ë¯¸ ë°ì´í„° ìƒì„±
            if len(filtered_docs) < 3:
                additional_docs = self._generate_type_specific_dummy_docs(query, keywords, user_type)
                filtered_docs.extend(additional_docs)
            
            return filtered_docs[:5]
            
        except Exception as e:
            # íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë”ë¯¸ ë°ì´í„°
            return self._generate_type_specific_dummy_docs(query, keywords, user_type)
    
    def _calculate_local_relevance(self, doc: Dict, query: str, keywords: List[str], user_type: str) -> float:
        """ë¡œì»¬ ë¬¸ì„œì˜ ê´€ë ¨ë„ ê³„ì‚°"""
        relevance = 0.0
        
        title = doc.get("title", "").lower()
        content = doc.get("content", "").lower()
        doc_keywords = [kw.lower() for kw in doc.get("keywords", [])]
        query_lower = query.lower()
        
        # ì¿¼ë¦¬ ë§¤ì¹­
        if query_lower in title:
            relevance += 0.4
        elif query_lower in content:
            relevance += 0.2
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        if keywords:
            keyword_matches = sum(1 for kw in keywords 
                                if kw.lower() in title or kw.lower() in content or kw.lower() in doc_keywords)
            relevance += (keyword_matches / len(keywords)) * 0.3
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ ë³´ë„ˆìŠ¤
        type_keywords = {
            "planner": ["ê¸°íš", "ì „ëµ", "ê³„íš", "ë°©ì•ˆ", "í”„ë¡œì íŠ¸"],
            "reporter": ["ë³´ê³ ì„œ", "ë¶„ì„", "ë°ì´í„°", "í†µê³„", "ì—°êµ¬"],
            "general": ["ì •ë³´", "ìë£Œ", "ë‚´ìš©"]
        }
        
        if user_type in type_keywords:
            type_matches = sum(1 for kw in type_keywords[user_type] 
                             if kw in title or kw in content)
            relevance += type_matches * 0.15
        
        return min(1.0, relevance)
    
    def _classify_local_document(self, doc: Dict, user_type: str) -> str:
        """ë¡œì»¬ ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜"""
        title = doc.get("title", "").lower()
        keywords = [kw.lower() for kw in doc.get("keywords", [])]
        
        if any(word in title for word in ["ë¶„ì„", "ë³´ê³ ì„œ", "ì—°êµ¬"]):
            return "ë¶„ì„ ë³´ê³ ì„œ"
        elif any(word in keywords for word in ["ê°€ì´ë“œ", "ë°©ë²•", "íŠœí† ë¦¬ì–¼"]):
            return "ì‹¤ìš© ê°€ì´ë“œ"
        elif user_type == "planner" and any(word in title for word in ["ê¸°íš", "ì „ëµ", "ê³„íš"]):
            return "ê¸°íš ìë£Œ"
        elif user_type == "reporter" and any(word in keywords for word in ["ë°ì´í„°", "í†µê³„"]):
            return "ë°ì´í„° ìë£Œ"
        else:
            return "ì°¸ê³  ìë£Œ"
    
    def _generate_type_specific_dummy_docs(self, query: str, keywords: List[str], user_type: str) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì íƒ€ì…ë³„ íŠ¹í™” ë”ë¯¸ ë¬¸ì„œ ìƒì„±"""
        base_docs = []
        
        if user_type == "planner":
            base_docs = [
                {
                    "id": "dummy_plan_1",
                    "title": f"'{query}' ê¸°íš ì‹¤í–‰ ê°€ì´ë“œ",
                    "summary": f"{query}ì™€ ê´€ë ¨ëœ ê¸°íš ì—…ë¬´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤.",
                    "content": f"ì´ ë¬¸ì„œëŠ” {query}ì— ëŒ€í•œ ê¸°íš í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¤ë£¨ë©°, í˜„í™© ë¶„ì„, ëª©í‘œ ì„¤ì •, ì‹¤í–‰ ê³„íš ìˆ˜ë¦½, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë“±ì˜ í•µì‹¬ ìš”ì†Œë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.",
                    "source": "ê¸°íš ì „ë¬¸ê°€ DB",
                    "relevance_score": 0.9,
                    "keywords": keywords[:5] if keywords else ["ê¸°íš", "ì „ëµ", "ì‹¤í–‰"],
                    "user_type": user_type,
                    "document_type": "ê¸°íš ê°€ì´ë“œ"
                },
                {
                    "id": "dummy_plan_2", 
                    "title": f"'{query}' í”„ë¡œì íŠ¸ ê´€ë¦¬ ì „ëµ",
                    "summary": f"{query} ê´€ë ¨ í”„ë¡œì íŠ¸ì˜ ì„±ê³µì  ê´€ë¦¬ë¥¼ ìœ„í•œ ì „ëµê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ì…ë‹ˆë‹¤.",
                    "content": f"í”„ë¡œì íŠ¸ ê´€ë¦¬ ê´€ì ì—ì„œ {query}ë¥¼ ë‹¤ë£¨ëŠ” ë°©ë²•ë¡ ê³¼ ì‹¤ë¬´ì§„ë“¤ì´ ì•Œì•„ì•¼ í•  í•µì‹¬ ì‚¬í•­ë“¤ì„ ì •ë¦¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.",
                    "source": "PM ì‹¤ë¬´ ê°€ì´ë“œ",
                    "relevance_score": 0.85,
                    "keywords": keywords[:5] if keywords else ["í”„ë¡œì íŠ¸", "ê´€ë¦¬", "ì „ëµ"],
                    "user_type": user_type,
                    "document_type": "ê´€ë¦¬ ì „ëµ"
                }
            ]
        elif user_type == "reporter":
            base_docs = [
                {
                    "id": "dummy_report_1",
                    "title": f"'{query}' í˜„í™© ë¶„ì„ ë³´ê³ ì„œ",
                    "summary": f"{query}ì— ëŒ€í•œ ì¢…í•©ì ì¸ í˜„í™© ë¶„ì„ê³¼ ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                    "content": f"ì´ ë³´ê³ ì„œëŠ” {query}ì— ëŒ€í•œ ì •ëŸ‰ì  ë¶„ì„ ê²°ê³¼ì™€ ì‹œì¥ ë™í–¥, ì£¼ìš” ì§€í‘œë“¤ì„ í¬í•¨í•˜ì—¬ ì˜ì‚¬ê²°ì •ì— í•„ìš”í•œ ê·¼ê±° ìë£Œë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                    "source": "ì‚°ì—… ë¶„ì„ ë¦¬í¬íŠ¸",
                    "relevance_score": 0.92,
                    "keywords": keywords[:5] if keywords else ["ë¶„ì„", "ë³´ê³ ì„œ", "ë°ì´í„°"],
                    "user_type": user_type,
                    "document_type": "ë¶„ì„ ë³´ê³ ì„œ"
                },
                {
                    "id": "dummy_report_2",
                    "title": f"'{query}' ë²¤ì¹˜ë§ˆí‚¹ ìŠ¤í„°ë””",
                    "summary": f"{query} ë¶„ì•¼ì˜ ì„ ë„ ê¸°ì—…ë“¤ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì‚¬ë¡€ ì—°êµ¬ ìë£Œì…ë‹ˆë‹¤.",
                    "content": f"ì—…ê³„ ì„ ë„ ê¸°ì—…ë“¤ì˜ {query} ê´€ë ¨ ì „ëµê³¼ ì„±ê³¼ë¥¼ ë¶„ì„í•œ ë²¤ì¹˜ë§ˆí‚¹ ì—°êµ¬ë¡œ, ë¹„êµ ë¶„ì„ ë°ì´í„°ì™€ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤.",
                    "source": "ë²¤ì¹˜ë§ˆí‚¹ ì—°êµ¬ì†Œ",
                    "relevance_score": 0.88,
                    "keywords": keywords[:5] if keywords else ["ë²¤ì¹˜ë§ˆí‚¹", "ì‚¬ë¡€", "ì—°êµ¬"],
                    "user_type": user_type,
                    "document_type": "ì—°êµ¬ ë³´ê³ ì„œ"
                }
            ]
        else:
            base_docs = [
                {
                    "id": "dummy_general_1",
                    "title": f"'{query}' ì¢…í•© ê°€ì´ë“œ",
                    "summary": f"{query}ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë¶€í„° ì‹¤ë¬´ ì ìš©ê¹Œì§€ í¬ê´„í•˜ëŠ” ì¢…í•© ìë£Œì…ë‹ˆë‹¤.",
                    "content": f"{query}ì˜ ê°œë…, ì¤‘ìš”ì„±, ì ìš© ë°©ë²• ë“±ì„ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ì¢…í•© ê°€ì´ë“œ ë¬¸ì„œì…ë‹ˆë‹¤.",
                    "source": "ì „ë¬¸ ìë£Œ DB",
                    "relevance_score": 0.8,
                    "keywords": keywords[:5] if keywords else ["ê°€ì´ë“œ", "ì •ë³´", "ìë£Œ"],
                    "user_type": user_type,
                    "document_type": "ì¢…í•© ê°€ì´ë“œ"
                }
            ]
        
        return base_docs
    
    def refine_text(self, text: str, style: str = "clear", user_type: str = "general") -> str:
        """í…ìŠ¤íŠ¸ ë‹¤ë“¬ê¸° - ì‚¬ìš©ì íƒ€ì…ë³„ ë§ì¶¤ ìŠ¤íƒ€ì¼"""
        if not self.ai_available:
            return self._get_dummy_refined_text(text, style, user_type)
        
        style_prompts = {
            "clear": "ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”. ë³µì¡í•œ ë¬¸ì¥ì€ ë‹¨ìˆœí™”í•˜ê³  ëª¨í˜¸í•œ í‘œí˜„ì€ êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
            "professional": "ì „ë¬¸ì ì´ê³  ì •í™•í•œ í‘œí˜„ìœ¼ë¡œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”. í•´ë‹¹ ë¶„ì•¼ì˜ ì ì ˆí•œ ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”.",
            "concise": "ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ìœ¼ë¡œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ì™€ ì¤‘ë³µ í‘œí˜„ì„ ì œê±°í•´ì£¼ì„¸ìš”.",
            "persuasive": "ì„¤ë“ë ¥ ìˆê³  ì„íŒ©íŠ¸ ìˆê²Œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”. ë…¼ë¦¬ì  ê·¼ê±°ì™€ ê°•ë ¥í•œ ë©”ì‹œì§€ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”."
        }
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ ì¶”ê°€ ì§€ì¹¨
        user_context = {
            "planner": "ê¸°íšì ê´€ì ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì¸ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
            "reporter": "ë³´ê³ ì„œ ì‘ì„±ì ê´€ì ì—ì„œ ê°ê´€ì ì´ê³  ë°ì´í„° ê¸°ë°˜ì˜ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
            "general": "ì¼ë°˜ ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ìˆ˜ì •í•´ì£¼ì„¸ìš”."
        }
        
        try:
            system_prompt = f"""
            {style_prompts.get(style, 'ë” ì¢‹ê²Œ')} 
            {user_context.get(user_type, '')}
            ì›ë³¸ì˜ ì˜ë¯¸ì™€ í•µì‹¬ ë‚´ìš©ì€ ë°˜ë“œì‹œ ìœ ì§€í•˜ë©´ì„œ ê°œì„ í•´ì£¼ì„¸ìš”.
            """
            
            response = self.client.chat.completions.create(
                model=AI_CONFIG["deployment_name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”:\n\n{text}"}
                ],
                max_tokens=min(AI_CONFIG["max_tokens"], len(text.split()) * 3),
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            st.error(f"í…ìŠ¤íŠ¸ ë‹¤ë“¬ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._get_dummy_refined_text(text, style, user_type)
    
    def _get_dummy_refined_text(self, text: str, style: str, user_type: str) -> str:
        """ë”ë¯¸ í…ìŠ¤íŠ¸ ë‹¤ë“¬ê¸° ê²°ê³¼"""
        if style == "clear":
            return f"[ëª…í™•ì„± ê°œì„ ] {text}\n\nâ†’ í•µì‹¬ ë‚´ìš©ì„ ë” ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ í‘œí˜„í–ˆìŠµë‹ˆë‹¤."
        elif style == "professional":
            return f"[ì „ë¬¸ì„± ê°•í™”] {text}\n\nâ†’ ì „ë¬¸ì ì¸ í‘œí˜„ê³¼ ì •í™•í•œ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤."
        elif style == "concise":
            return f"[ê°„ê²°ì„± ê°œì„ ] {text[:len(text)//2]}...\n\nâ†’ ë¶ˆí•„ìš”í•œ í‘œí˜„ì„ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‚¨ê²¼ìŠµë‹ˆë‹¤."
        else:
            return f"[{style} ìŠ¤íƒ€ì¼] {text}\n\nâ†’ {style} ìŠ¤íƒ€ì¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    def structure_content(self, text: str, structure_type: str = "outline", user_type: str = "general") -> str:
        """ë‚´ìš© êµ¬ì¡°í™” - ì‚¬ìš©ì íƒ€ì…ë³„ ë§ì¶¤ êµ¬ì¡°"""
        if not self.ai_available:
            return self._get_dummy_structured_content(text, structure_type, user_type)
        
        structure_prompts = {
            "outline": "ë‹¤ìŒ ë‚´ìš©ì„ ëª©ì°¨ì™€ ì†Œì œëª©ì´ ìˆëŠ” ì²´ê³„ì ì¸ ê°œìš” í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "steps": "ë‹¤ìŒ ë‚´ìš©ì„ ë‹¨ê³„ë³„ ê°€ì´ë“œ í˜•ì‹(1ë‹¨ê³„, 2ë‹¨ê³„...)ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "qa": "ë‹¤ìŒ ë‚´ìš©ì„ ì§ˆë¬¸ê³¼ ë‹µë³€(Q&A) í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "summary": "ë‹¤ìŒ ë‚´ìš©ì„ í•µì‹¬ ìš”ì•½, ìƒì„¸ ë‚´ìš©, ê²°ë¡  í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "presentation": "ë‹¤ìŒ ë‚´ìš©ì„ í”„ë ˆì  í…Œì´ì…˜ìš© ìŠ¬ë¼ì´ë“œ êµ¬ì¡°ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
        }
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ êµ¬ì¡°í™” ì§€ì¹¨
        user_guidance = {
            "planner": "ì‹¤í–‰ ê³„íšê³¼ ì•¡ì…˜ ì•„ì´í…œì„ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "reporter": "ë°ì´í„°ì™€ ê·¼ê±°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "general": "ë…ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ë…¼ë¦¬ì  íë¦„ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”."
        }
        
        try:
            system_prompt = f"""
            {structure_prompts.get(structure_type, "ë‹¤ìŒ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.")}
            {user_guidance.get(user_type, '')}
            
            êµ¬ì¡°í™” ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
            - ë…¼ë¦¬ì  íë¦„ê³¼ ì—°ê²°ì„±
            - í•µì‹¬ ë‚´ìš©ì˜ ê°•ì¡°
            - ì½ê¸° ì‰¬ìš´ í˜•íƒœ
            """
            
            response = self.client.chat.completions.create(
                model=AI_CONFIG["deployment_name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                max_tokens=AI_CONFIG["max_tokens"],
                temperature=0.4
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            st.error(f"ë‚´ìš© êµ¬ì¡°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._get_dummy_structured_content(text, structure_type, user_type)
    
    def _get_dummy_structured_content(self, text: str, structure_type: str, user_type: str) -> str:
        """ë”ë¯¸ êµ¬ì¡°í™” ê²°ê³¼"""
        text_preview = text[:100] + "..." if len(text) > 100 else text
        
        if structure_type == "outline":
            return f"""# ì£¼ì œ ê°œìš”

## 1. ë°°ê²½ ë° í˜„í™©
{text_preview}

## 2. ì£¼ìš” ë‚´ìš©
- í•µì‹¬ í¬ì¸íŠ¸ 1: ì£¼ìš” ì´ìŠˆ ë¶„ì„
- í•µì‹¬ í¬ì¸íŠ¸ 2: í•´ê²° ë°©ì•ˆ ëª¨ìƒ‰  
- í•µì‹¬ í¬ì¸íŠ¸ 3: ê¸°ëŒ€ íš¨ê³¼

## 3. ê²°ë¡  ë° ì œì–¸
- ìš”ì•½ ì •ë¦¬
- í–¥í›„ ê³„íš
- ê¶Œê³ ì‚¬í•­

[ë”ë¯¸ ëª¨ë“œë¡œ ìƒì„±ëœ êµ¬ì¡°ì…ë‹ˆë‹¤]"""
        
        elif structure_type == "steps":
            return f"""# ë‹¨ê³„ë³„ ì‹¤í–‰ ê°€ì´ë“œ

**1ë‹¨ê³„: í˜„í™© íŒŒì•…**
- {text_preview}
- ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„

**2ë‹¨ê³„: ê³„íš ìˆ˜ë¦½**
- ëª©í‘œ ì„¤ì • ë° ìš°ì„ ìˆœìœ„ ê²°ì •
- ë¦¬ì†ŒìŠ¤ ë° ì¼ì • ê³„íš

**3ë‹¨ê³„: ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§**
- ë‹¨ê³„ë³„ ì‹¤í–‰
- ì§„í–‰ ìƒí™© ì ê²€

**4ë‹¨ê³„: í‰ê°€ ë° ê°œì„ **
- ê²°ê³¼ ë¶„ì„
- ê°œì„  ë°©ì•ˆ ë„ì¶œ

[ë”ë¯¸ ëª¨ë“œë¡œ ìƒì„±ëœ ë‹¨ê³„ì…ë‹ˆë‹¤]"""
        
        elif structure_type == "qa":
            return f"""# Q&A í˜•ì‹ ì •ë¦¬

**Q1: í•µì‹¬ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€?**
A: {text_preview}

**Q2: ì–´ë–¤ ì ‘ê·¼ ë°©ë²•ì´ í•„ìš”í•œê°€?**
A: ì²´ê³„ì ì¸ ë¶„ì„ê³¼ ë‹¨ê³„ë³„ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.

**Q3: ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼ëŠ”?**
A: íš¨ê³¼ì ì¸ ë¬¸ì œ í•´ê²°ê³¼ ì„±ê³¼ ê°œì„ ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Q4: ì£¼ì˜ì‚¬í•­ì€?**
A: ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ í”¼ë“œë°±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

[ë”ë¯¸ ëª¨ë“œë¡œ ìƒì„±ëœ Q&Aì…ë‹ˆë‹¤]"""
        
        else:
            return f"""# êµ¬ì¡°í™”ëœ ë‚´ìš©

## í•µì‹¬ ìš”ì•½
{text_preview}

## ìƒì„¸ ë‚´ìš©
- ì£¼ìš” ìš”ì†Œë“¤ì˜ êµ¬ì²´ì  ì„¤ëª…
- ê´€ë ¨ ë°ì´í„° ë° ê·¼ê±°
- ì‹¤ë¬´ ì ìš© ë°©ì•ˆ

## ê²°ë¡ 
ìš”ì•½ëœ ê²°ë¡ ê³¼ í–¥í›„ ë°©í–¥ì„±

[ë”ë¯¸ ëª¨ë“œë¡œ ìƒì„±ëœ êµ¬ì¡°ì…ë‹ˆë‹¤]"""
    
    def generate_action_plan(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì•¡ì…˜ í”Œëœ ìƒì„±"""
        if not self.ai_available:
            return self._get_dummy_action_plan(analysis_result)
        
        try:
            text = analysis_result.get("original_text", "")
            user_type = analysis_result.get("user_type", "general")
            analysis = analysis_result.get("analysis", {})
            
            system_prompt = f"""
            ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ í”Œëœì„ ìƒì„±í•´ì£¼ì„¸ìš”.
            
            ì‚¬ìš©ì íƒ€ì…: {user_type}
            
            ì•¡ì…˜ í”Œëœì—ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ì¦‰ì‹œ ì‹¤í–‰ í•­ëª© (1-3ê°œ)
            2. ë‹¨ê¸° ê³„íš (1ì£¼-1ê°œì›”)
            3. ì¤‘ì¥ê¸° ê³„íš (1-6ê°œì›”)
            4. í•„ìš” ë¦¬ì†ŒìŠ¤
            5. ì„±ê³µ ì§€í‘œ
            """
            
            response = self.client.chat.completions.create(
                model=AI_CONFIG["deployment_name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ë¶„ì„ ë‚´ìš©: {text}\n\në¶„ì„ ê²°ê³¼: {analysis}"}
                ],
                max_tokens=AI_CONFIG["max_tokens"],
                temperature=0.5
            )
            
            content = response.choices[0].message.content
            
            return {
                "action_plan": content,
                "user_type": user_type,
                "generated_at": time.time()
            }
            
        except Exception as e:
            st.error(f"ì•¡ì…˜ í”Œëœ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._get_dummy_action_plan(analysis_result)
    
    def _get_dummy_action_plan(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë”ë¯¸ ì•¡ì…˜ í”Œëœ"""
        user_type = analysis_result.get("user_type", "general")
        
        if user_type == "planner":
            plan = """# ê¸°íš ì‹¤í–‰ ì•¡ì…˜ í”Œëœ

## ğŸš€ ì¦‰ì‹œ ì‹¤í–‰ í•­ëª©
1. í˜„í™© ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘
2. í•µì‹¬ ì´í•´ê´€ê³„ì ë¯¸íŒ… ì¼ì • ì¡°ìœ¨
3. í”„ë¡œì íŠ¸ ë²”ìœ„ ë° ëª©í‘œ ëª…í™•í™”

## ğŸ“… ë‹¨ê¸° ê³„íš (1ì£¼-1ê°œì›”)
- ìƒì„¸ ê¸°íšì„œ ì‘ì„±
- ì˜ˆì‚° ë° ë¦¬ì†ŒìŠ¤ ê³„íš ìˆ˜ë¦½
- íŒ€ êµ¬ì„± ë° ì—­í•  ë°°ì •

## ğŸ¯ ì¤‘ì¥ê¸° ê³„íš (1-6ê°œì›”)
- ë‹¨ê³„ë³„ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§
- ì¤‘ê°„ í‰ê°€ ë° ì¡°ì •
- ì„±ê³¼ ì¸¡ì • ë° ë³´ê³ 

## ğŸ“Š í•„ìš” ë¦¬ì†ŒìŠ¤
- ì¸ë ¥: ê¸°íšíŒ€, ì‹¤í–‰íŒ€
- ì˜ˆì‚°: í”„ë¡œì íŠ¸ ê·œëª¨ì— ë”°ë¼
- ì‹œìŠ¤í…œ: í˜‘ì—… ë„êµ¬, ë¶„ì„ ë„êµ¬

## âœ… ì„±ê³µ ì§€í‘œ
- ëª©í‘œ ë‹¬ì„±ë¥ 
- ì¼ì • ì¤€ìˆ˜ìœ¨
- í’ˆì§ˆ ë§Œì¡±ë„"""
        
        elif user_type == "reporter":
            plan = """# ë³´ê³ ì„œ ì‘ì„± ì•¡ì…˜ í”Œëœ

## ğŸš€ ì¦‰ì‹œ ì‹¤í–‰ í•­ëª©
1. ë³´ê³ ì„œ ëª©ì ê³¼ ëŒ€ìƒ ë…ì ëª…í™•í™”
2. í•„ìš” ë°ì´í„° ë° ìë£Œ ë¦¬ìŠ¤íŠ¸ ì‘ì„±
3. ë³´ê³ ì„œ êµ¬ì¡° ë° ëª©ì°¨ ì´ˆì•ˆ ì‘ì„±

## ğŸ“… ë‹¨ê¸° ê³„íš (1ì£¼-1ê°œì›”)
- ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
- ì´ˆì•ˆ ì‘ì„± ë° ê²€í† 
- ì‹œê° ìë£Œ(ì°¨íŠ¸, ê·¸ë˜í”„) ì œì‘

## ğŸ¯ ì¤‘ì¥ê¸° ê³„íš (1-6ê°œì›”)
- ì •ê¸° ë³´ê³ ì„œ í…œí”Œë¦¿ ê°œë°œ
- ë°ì´í„° ìˆ˜ì§‘ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•
- ë³´ê³ ì„œ í’ˆì§ˆ ê°œì„  í”„ë¡œì„¸ìŠ¤ ì •ë¦½

## ğŸ“Š í•„ìš” ë¦¬ì†ŒìŠ¤
- ë°ì´í„°: ê´€ë ¨ í†µê³„, ë¶„ì„ ìë£Œ
- ë„êµ¬: ë¶„ì„ ì†Œí”„íŠ¸ì›¨ì–´, ë””ìì¸ íˆ´
- ì‹œê°„: ì¶©ë¶„í•œ ê²€í†  ë° ìˆ˜ì • ê¸°ê°„

## âœ… ì„±ê³µ ì§€í‘œ
- ë³´ê³ ì„œ ì™„ì„±ë„
- ë°ì´í„° ì •í™•ì„±
- ë…ì ë§Œì¡±ë„"""
        
        else:
            plan = """# ì¢…í•© ì•¡ì…˜ í”Œëœ

## ğŸš€ ì¦‰ì‹œ ì‹¤í–‰ í•­ëª©
1. í•µì‹¬ ì´ìŠˆ ì •ë¦¬ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •
2. ê´€ë ¨ ìë£Œ ë° ì •ë³´ ìˆ˜ì§‘
3. ì‹¤í–‰ íŒ€ êµ¬ì„± ë° ì—­í•  ë¶„ë‹´

## ğŸ“… ë‹¨ê¸° ê³„íš (1ì£¼-1ê°œì›”)
- ì„¸ë¶€ ê³„íš ìˆ˜ë¦½
- ì´ˆê¸° ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸
- í”¼ë“œë°± ìˆ˜ì§‘ ë° ê°œì„ 

## ğŸ¯ ì¤‘ì¥ê¸° ê³„íš (1-6ê°œì›”)
- ë³¸ê²©ì ì¸ ì‹¤í–‰ ë‹¨ê³„
- ì„±ê³¼ ëª¨ë‹ˆí„°ë§ ë° ì¡°ì •
- ìµœì¢… í‰ê°€ ë° ì •ë¦¬

## ğŸ“Š í•„ìš” ë¦¬ì†ŒìŠ¤
- ì¸ë ¥: ë‹¤ì–‘í•œ ë¶„ì•¼ ì „ë¬¸ê°€
- ë„êµ¬: ì—…ë¬´ íš¨ìœ¨ì„± ë„êµ¬
- ì˜ˆì‚°: í”„ë¡œì íŠ¸ ê·œëª¨ë³„ ì ì • ì˜ˆì‚°

## âœ… ì„±ê³µ ì§€í‘œ
- ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
- íš¨ìœ¨ì„± ê°œì„ ë„
- ë§Œì¡±ë„ í‰ê°€"""
        
        return {
            "action_plan": plan,
            "user_type": user_type,
            "generated_at": time.time()
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜"""
        if not text.strip():
            return []
        
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (í•œêµ­ì–´)
        stop_words = {
            'ìˆëŠ”', 'ìˆì–´', 'ìˆë‹¤', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ê·¸ëŸ°ë°',
            'ì´ëŠ”', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°ì„œ', 'ê±°ê¸°ì„œ', 'ì €ê¸°ì„œ', 'ë•Œë¬¸ì—',
            'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'í•˜ëŠ”', 'í•˜ê³ ', 'í•œë‹¤', 'ë©ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤.',
            'ê²ƒì„', 'ê²ƒì´', 'ê²ƒì€', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'ì˜', 'ë¥¼', 'ì„',
            'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ê³¼', 'ì™€', 'ë„', 'ë§Œ',
            'ë¶€í„°', 'ê¹Œì§€', 'ìœ„í•´', 'ëŒ€í•´', 'ëŒ€í•œ', 'í†µí•´', 'ìœ„í•œ', 'ê°™ì€', 'ë‹¤ë¥¸'
        }
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        import re
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        words = text.split()
        
        # í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ
        keyword_candidates = []
        for word in words:
            clean_word = word.strip().lower()
            
            # ê¸¸ì´ ì œí•œ ë° ë¶ˆìš©ì–´ ì œê±°
            if (len(clean_word) > 2 and 
                clean_word not in stop_words and
                not clean_word.isdigit()):
                keyword_candidates.append(clean_word)
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        from collections import Counter
        word_freq = Counter(keyword_candidates)
        
        # ìƒìœ„ ë¹ˆë„ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœëŒ€ 10ê°œ)
        top_keywords = [word for word, freq in word_freq.most_common(10)]
        
        return top_keywords
    
    def _extract_topic(self, content: str) -> str:
        """ì£¼ì œ ì¶”ì¶œ - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜"""
        if not content:
            return "ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨"
        
        lines = content.split('\n')
        
        # ì£¼ì œë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” íŒ¨í„´ë“¤
        topic_patterns = [
            r'ì£¼ì œ[:\s]*(.+)',
            r'topic[:\s]*(.+)',
            r'ì œëª©[:\s]*(.+)',
            r'title[:\s]*(.+)',
            r'í•µì‹¬[:\s]*(.+)',
            r'#\s*(.+)'  # ë§ˆí¬ë‹¤ìš´ ì œëª©
        ]
        
        for line in lines[:5]:  # ì²« 5ì¤„ì—ì„œ ì°¾ê¸°
            line = line.strip()
            for pattern in topic_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    topic = match.group(1).strip()
                    if len(topic) > 5 and len(topic) < 100:  # ì ì ˆí•œ ê¸¸ì´
                        return topic
        
        # íŒ¨í„´ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ ë¬¸ì¥ ì‚¬ìš©
        sentences = content.split('.')
        for sentence in sentences[:3]:
            sentence = sentence.strip()
            if len(sentence) > 10 and len(sentence) < 100:
                return sentence + "."
        
        return "ì£¼ì œ ë¶„ì„ í•„ìš”"
    
    def _extract_summary(self, content: str) -> str:
        """ìš”ì•½ ì¶”ì¶œ - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜"""
        if not content:
            return "ìš”ì•½ ì—†ìŒ"
        
        lines = content.split('\n')
        
        # ìš”ì•½ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” íŒ¨í„´ë“¤
        summary_patterns = [
            r'ìš”ì•½[:\s]*(.+)',
            r'summary[:\s]*(.+)', 
            r'í•µì‹¬[:\s]*(.+)',
            r'ê²°ë¡ [:\s]*(.+)',
            r'ê°œìš”[:\s]*(.+)'
        ]
        
        summary_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            for pattern in summary_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    summary_lines.append(match.group(1).strip())
        
        if summary_lines:
            summary = ' '.join(summary_lines)
            if len(summary) > 20:
                return summary[:200] + "..." if len(summary) > 200 else summary
        
        # íŒ¨í„´ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° ë‚´ìš©ì˜ ì²˜ìŒ ë¶€ë¶„ ì‚¬ìš©
        sentences = content.split('.')
        key_sentences = []
        
        for sentence in sentences[:5]:
            sentence = sentence.strip()
            if len(sentence) > 20:  # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ ê¸¸ì´
                key_sentences.append(sentence)
                if len(' '.join(key_sentences)) > 150:
                    break
        
        if key_sentences:
            summary = '. '.join(key_sentences[:2]) + "."
            return summary[:200] + "..." if len(summary) > 200 else summary
        
        return "ìš”ì•½ ìƒì„± í•„ìš”"
