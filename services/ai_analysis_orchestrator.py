"""
ai_analysis_orchestrator.py

AI ë¶„ì„ 4ë‹¨ê³„ ìˆœì°¨ í”„ë¡œì„¸ìŠ¤ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
- 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ê³ ë„í™”
- 2ë‹¨ê³„: ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
- 3ë‹¨ê³„: ì‚¬ë‚´/ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤ ê²€ìƒ‰ (ë³‘ë ¬)
- 4ë‹¨ê³„: ìµœì¢… ë¶„ì„ ê²°ê³¼ ìƒì„±

ë¶„ì„ ëª¨ë“œ: ì „ì²´ ë¬¸ì„œ/ì„ íƒ í…ìŠ¤íŠ¸
ì§„í–‰ ìƒí™© í‘œì‹œ, ë ˆí¼ëŸ°ìŠ¤ ê´€ë¦¬, ë¶„ì„ ì·¨ì†Œ ë“± ì§€ì›
"""
import os
import threading
from typing import List, Dict, Literal, Optional, Any
import time

from utils.azure_search_management import AzureSearchService
import requests
import openai
import streamlit as st
from config import AI_CONFIG

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ë¡œë“œ
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "")
AZURE_OPENAI_DEPLOYMENT = AI_CONFIG["deployment_name"]

class AIAnalysisOrchestrator:
    def __init__(self, mode: Literal["full", "selection"] = "full"):
        self.mode = mode
        self.azure_search = AzureSearchService()
        self.cancelled = False
        self.progress = 0
        self.status = ""
        self.references: Dict[str, List[Dict[str, Any]]] = {"internal": [], "external": []}
        self.result: Optional[str] = None
        self.lock = threading.Lock()
        
        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = openai.AzureOpenAI(
            azure_endpoint=AI_CONFIG["openai_endpoint"],
            api_key=AI_CONFIG["openai_api_key"],
            api_version=AI_CONFIG["api_version"]
        )

    def cancel(self):
        with self.lock:
            self.cancelled = True

    def is_cancelled(self):
        with self.lock:
            return self.cancelled

    def run_analysis(self, user_input: str, selection: Optional[str] = None):
        self.progress = 0
        self.status = "1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ê³ ë„í™”"
        st.session_state["ai_analysis_progress"] = self.progress
        st.session_state["ai_analysis_status"] = self.status
        st.session_state["ai_analysis_cancelled"] = False
        st.session_state["ai_analysis_result"] = None
        st.session_state["ai_analysis_references"] = {"internal": [], "external": []}

        # 1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ê³ ë„í™”
        prompt = self._refine_prompt(user_input, selection)
        self.progress = 20
        st.session_state["ai_analysis_progress"] = self.progress
        st.session_state["ai_analysis_status"] = "2ë‹¨ê³„: ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±"
        if self.is_cancelled():
            return

        # 2ë‹¨ê³„: ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
        internal_query, external_query = self._generate_queries(prompt)
        self.progress = 40
        st.session_state["ai_analysis_progress"] = self.progress
        st.session_state["ai_analysis_status"] = "3ë‹¨ê³„: ì‚¬ë‚´/ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤ ê²€ìƒ‰"
        if self.is_cancelled():
            return

        # 3ë‹¨ê³„: ì‚¬ë‚´/ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤ ê²€ìƒ‰ (ë³‘ë ¬)
        internal_refs, external_refs = self._parallel_reference_search(internal_query, external_query)
        self.references = {"internal": internal_refs, "external": external_refs}
        st.session_state["ai_analysis_references"] = self.references
        self.progress = 70
        st.session_state["ai_analysis_progress"] = self.progress
        st.session_state["ai_analysis_status"] = "4ë‹¨ê³„: ìµœì¢… ë¶„ì„ ê²°ê³¼ ìƒì„±"
        if self.is_cancelled():
            return

        # 4ë‹¨ê³„: ìµœì¢… ë¶„ì„ ê²°ê³¼ ìƒì„±
        self.result = self._generate_final_result(prompt, internal_refs, external_refs)
        st.session_state["ai_analysis_result"] = self.result
        self.progress = 100
        st.session_state["ai_analysis_progress"] = self.progress
        st.session_state["ai_analysis_status"] = "ë¶„ì„ ì™„ë£Œ"

    def _refine_prompt(self, user_input: str, selection: Optional[str]) -> str:
        """1ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ê³ ë„í™” - GPT-4oë¥¼ í™œìš©í•œ ì‚¬ìš©ì ì…ë ¥ ê°œì„ """
        try:
            system_prompt = """ë‹¹ì‹ ì€ AI ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ í•´ì£¼ì„¸ìš”.

ê°œì„  ê¸°ì¤€:
1. ë¶„ì„ ëª©ì ê³¼ ë°©í–¥ì„±ì„ ëª…í™•íˆ í•˜ê¸°
2. ê²€ìƒ‰ì— ì í•©í•œ í‚¤ì›Œë“œ í¬í•¨
3. êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì§ˆë¬¸ìœ¼ë¡œ ë³€í™˜
4. ì‚¬ë‚´ ë¬¸ì„œì™€ ì™¸ë¶€ ìë£Œ ëª¨ë‘ì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆë„ë¡ êµ¬ì„±

ì…ë ¥ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

            user_prompt = f"ì‚¬ìš©ì ì…ë ¥: {user_input}"
            if self.mode == "selection" and selection:
                user_prompt += f"\n\në¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸: {selection}"

            response = self.openai_client.chat.completions.create(
                model=AZURE_OPENAI_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            enhanced_prompt = response.choices[0].message.content.strip()
            return enhanced_prompt
        
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
            base_prompt = f"ë¶„ì„ ëª©ì : {user_input}"
            if self.mode == "selection" and selection:
                base_prompt += f"\në¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸: {selection}"
            return base_prompt

    def _generate_queries(self, prompt: str) -> tuple[str, str]:
        """2ë‹¨ê³„: ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„± - ì‚¬ë‚´/ì™¸ë¶€ ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ ìƒì„±"""
        try:
            system_prompt = """ë‹¹ì‹ ì€ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í”„ë¡¬í”„íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‘ ê°€ì§€ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

1. ì‚¬ë‚´ ë¬¸ì„œ ê²€ìƒ‰ìš©: ì¡°ì§ ë‚´ë¶€ì˜ ë¬¸ì„œ, ì •ì±…, ê°€ì´ë“œë¼ì¸ ë“±ì„ ì°¾ê¸° ìœ„í•œ ì¿¼ë¦¬
2. ì™¸ë¶€ ìë£Œ ê²€ìƒ‰ìš©: ì›¹ì—ì„œ ìµœì‹  ì •ë³´, ì—…ê³„ ë™í–¥, ê¸°ìˆ  ìë£Œ ë“±ì„ ì°¾ê¸° ìœ„í•œ ì¿¼ë¦¬

ê° ì¿¼ë¦¬ëŠ” í•´ë‹¹ ê²€ìƒ‰ í™˜ê²½ì— ìµœì í™”ë˜ì–´ì•¼ í•˜ë©°, í•µì‹¬ í‚¤ì›Œë“œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.

ì¶œë ¥ í˜•ì‹:
ì‚¬ë‚´ê²€ìƒ‰: [ì‚¬ë‚´ ë¬¸ì„œ ê²€ìƒ‰ ì¿¼ë¦¬]
ì™¸ë¶€ê²€ìƒ‰: [ì™¸ë¶€ ìë£Œ ê²€ìƒ‰ ì¿¼ë¦¬]"""

            response = self.openai_client.chat.completions.create(
                model=AZURE_OPENAI_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"í”„ë¡¬í”„íŠ¸: {prompt}"}
                ],
                max_tokens=300,
                temperature=0.3
            )
            
            result = response.choices[0].message.content.strip()
            
            # ê²°ê³¼ íŒŒì‹±
            internal_query = ""
            external_query = ""
            
            for line in result.split('\n'):
                if line.startswith('ì‚¬ë‚´ê²€ìƒ‰:'):
                    internal_query = line.replace('ì‚¬ë‚´ê²€ìƒ‰:', '').strip()
                elif line.startswith('ì™¸ë¶€ê²€ìƒ‰:'):
                    external_query = line.replace('ì™¸ë¶€ê²€ìƒ‰:', '').strip()
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’
            if not internal_query:
                internal_query = prompt
            if not external_query:
                external_query = prompt
                
            return internal_query, external_query
            
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¿¼ë¦¬ ë°˜í™˜
            return prompt, prompt

    def _parallel_reference_search(self, internal_query: str, external_query: str):
        internal_refs, external_refs = [], []
        threads = []
        def search_internal():
            nonlocal internal_refs
            internal_refs = self._search_internal(internal_query)
        def search_external():
            nonlocal external_refs
            external_refs = self._search_external(external_query)
        t1 = threading.Thread(target=search_internal)
        t2 = threading.Thread(target=search_external)
        t1.start(); t2.start(); t1.join(); t2.join()
        return internal_refs, external_refs

    def _search_internal(self, query: str) -> List[Dict[str, Any]]:
        """ì‚¬ë‚´ ë¬¸ì„œ ê²€ìƒ‰: Azure AI Searchë¥¼ í†µí•œ ë²¡í„°/í‚¤ì›Œë“œ ê²€ìƒ‰"""
        try:
            if not self.azure_search.available:
                return [{"title": "Azure Search ì„œë¹„ìŠ¤ ì´ìš©ë¶ˆê°€", "content": "Azure AI Search ì„œë¹„ìŠ¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "url": ""}]
            
            # Azure AI Searchì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
            search_results = self.azure_search.search_documents(query, top=5)
            
            # ê²°ê³¼ë¥¼ í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜
            formatted_results = []
            for doc in search_results:
                formatted_results.append({
                    "title": doc.get("title", doc.get("filename", "ì œëª©ì—†ìŒ")),
                    "content": doc.get("content", doc.get("summary", ""))[:500],  # 500ì ì œí•œ
                    "url": doc.get("blob_url", ""),
                    "source": "internal",
                    "score": doc.get("@search.score", 0)
                })
            
            return formatted_results
            
        except Exception as e:
            return [{"title": "ì‚¬ë‚´ ê²€ìƒ‰ ì˜¤ë¥˜", "content": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "url": "", "source": "internal"}]

    def _search_external(self, query: str) -> List[Dict[str, Any]]:
        """ì™¸ë¶€ ìë£Œ ê²€ìƒ‰: Tavily APIë¥¼ í†µí•œ ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰"""
        try:
            if not TAVILY_API_KEY:
                return [{"title": "Tavily API í‚¤ ì—†ìŒ", "content": "TAVILY_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "url": "", "source": "external"}]
            
            # Tavily API í˜¸ì¶œ
            response = requests.post(
                "https://api.tavily.com/search",
                json={
                    "api_key": TAVILY_API_KEY,
                    "query": query,
                    "search_depth": "advanced",
                    "include_answer": True,
                    "include_raw_content": False,
                    "max_results": 5
                },
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                results = data.get("results", [])
                
                # ê²°ê³¼ë¥¼ í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë³€í™˜
                formatted_results = []
                for item in results:
                    formatted_results.append({
                        "title": item.get("title", "ì œëª©ì—†ìŒ"),
                        "content": item.get("content", "")[:500],  # 500ì ì œí•œ
                        "url": item.get("url", ""),
                        "source": "external",
                        "score": item.get("score", 0)
                    })
                
                return formatted_results
            else:
                return [{"title": "Tavily API ì˜¤ë¥˜", "content": f"HTTP {response.status_code}: {response.text}", "url": "", "source": "external"}]
                
        except requests.exceptions.Timeout:
            return [{"title": "Tavily ê²€ìƒ‰ ì‹œê°„ì´ˆê³¼", "content": "ì™¸ë¶€ ê²€ìƒ‰ ìš”ì²­ì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.", "url": "", "source": "external"}]
        except Exception as e:
            return [{"title": "Tavily ê²€ìƒ‰ ì˜ˆì™¸", "content": f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", "url": "", "source": "external"}]

    def _generate_final_result(self, prompt: str, internal_refs: List[Dict[str, Any]], external_refs: List[Dict[str, Any]]) -> str:
        """4ë‹¨ê³„: ìµœì¢… ë¶„ì„ ê²°ê³¼ ìƒì„± - ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•œ AI ë¶„ì„"""
        try:
            # ë ˆí¼ëŸ°ìŠ¤ ì •ë³´ ì •ë¦¬
            internal_context = ""
            for ref in internal_refs[:3]:  # ìƒìœ„ 3ê°œë§Œ
                internal_context += f"- {ref.get('title', 'ì œëª©ì—†ìŒ')}: {ref.get('content', '')[:200]}...\n"
            
            external_context = ""
            for ref in external_refs[:3]:  # ìƒìœ„ 3ê°œë§Œ
                external_context += f"- {ref.get('title', 'ì œëª©ì—†ìŒ')}: {ref.get('content', '')[:200]}...\n"

            system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ AI ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ ìˆ˜ì§‘ëœ ì‚¬ë‚´ ë¬¸ì„œ ë° ì™¸ë¶€ ìë£Œë¥¼ ì¢…í•©í•˜ì—¬ ì™„ì „í•˜ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:
1. ğŸ¯ ë¶„ì„ ìš”ì•½
2. ğŸ“‹ ì£¼ìš” ë°œê²¬ì‚¬í•­ (3-5ê°œ)
3. ğŸ’¡ ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆì‚¬í•­
4. âš ï¸ ì£¼ì˜ì‚¬í•­ ë° ê³ ë ¤ì‚¬í•­
5. ğŸ”— ì°¸ê³ ìë£Œ ìš”ì•½

ì‚¬ë‚´ ë¬¸ì„œì™€ ì™¸ë¶€ ìë£Œì˜ ì •ë³´ë¥¼ ê· í˜•ìˆê²Œ í™œìš©í•˜ê³ , êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë‚´ìš©ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            user_prompt = f"""
ë¶„ì„ ìš”ì²­: {prompt}

ì‚¬ë‚´ ë¬¸ì„œ ì •ë³´:
{internal_context if internal_context else "ì‚¬ë‚´ ë¬¸ì„œ ì—†ìŒ"}

ì™¸ë¶€ ìë£Œ ì •ë³´:
{external_context if external_context else "ì™¸ë¶€ ìë£Œ ì—†ìŒ"}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""

            response = self.openai_client.chat.completions.create(
                model=AZURE_OPENAI_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2000,
                temperature=0.4
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ìš”ì•½ ë°˜í™˜
            summary = f"## ğŸ“Š AI ë¶„ì„ ê²°ê³¼\n\n"
            summary += f"**ë¶„ì„ ìš”ì²­**: {prompt}\n\n"
            summary += f"**ìˆ˜ì§‘ëœ ìë£Œ**:\n"
            summary += f"- ì‚¬ë‚´ ë¬¸ì„œ: {len(internal_refs)}ê±´\n"
            summary += f"- ì™¸ë¶€ ìë£Œ: {len(external_refs)}ê±´\n\n"
            summary += f"**ì˜¤ë¥˜ ì •ë³´**: ìµœì¢… ë¶„ì„ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({str(e)})\n\n"
            
            if internal_refs:
                summary += "**ì‚¬ë‚´ ë¬¸ì„œ ì˜ˆì‹œ**:\n"
                summary += f"- {internal_refs[0].get('title', 'ì œëª©ì—†ìŒ')}: {internal_refs[0].get('content', '')[:200]}...\n\n"
            
            if external_refs:
                summary += "**ì™¸ë¶€ ìë£Œ ì˜ˆì‹œ**:\n" 
                summary += f"- {external_refs[0].get('title', 'ì œëª©ì—†ìŒ')}: {external_refs[0].get('content', '')[:200]}...\n"
            
            return summary

# Streamlit ì—°ë™ í•¨ìˆ˜ (UI/ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬)
def run_ai_analysis(user_input: str, mode: Literal["full", "selection"] = "full", selection: Optional[str] = None):
    """AI ë¶„ì„ì„ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰í•˜ê³  ì„¸ì…˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸"""
    
    def analysis_worker():
        orchestrator = AIAnalysisOrchestrator(mode=mode)
        try:
            orchestrator.run_analysis(user_input, selection)
        except Exception as e:
            st.session_state["ai_analysis_status"] = f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            st.session_state["ai_analysis_progress"] = 0
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë¶„ì„ ì‹¤í–‰
    if "ai_analysis_thread" not in st.session_state or not st.session_state["ai_analysis_thread"].is_alive():
        st.session_state["ai_analysis_cancelled"] = False
        st.session_state["ai_analysis_progress"] = 0
        st.session_state["ai_analysis_status"] = "ë¶„ì„ ì‹œì‘..."
        
        analysis_thread = threading.Thread(target=analysis_worker)
        st.session_state["ai_analysis_thread"] = analysis_thread
        analysis_thread.start()
        
        # ì•½ê°„ì˜ ì§€ì—° í›„ UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ rerun
        time.sleep(0.5)
        st.rerun()
