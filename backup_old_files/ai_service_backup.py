"""
AI ì„œë¹„ìŠ¤ ëª¨ë“ˆ - Azure OpenAIì™€ Tavily ê²€ìƒ‰ì„ ì‚¬ìš©
ê¸°íšìì™€ ë³´ê³ ì„œ ì œì‘ìë¥¼ ìœ„í•œ ì „ë¬¸ ë¶„ì„ ë„êµ¬
"""
import openai
import time
import streamlit as st
import json
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Tuple
from tavily import TavilyClient
from config import AI_CONFIG, TAVILY_CONFIG, LANGSMITH_CONFIG, AZURE_SEARCH_CONFIG

# LangSmith ì¶”ì  ì„¤ì •
try:
    if LANGSMITH_CONFIG["enabled"]:
        from langsmith import Client, trace
        import os
        os.environ["LANGCHAIN_API_KEY"] = LANGSMITH_CONFIG["api_key"]
        os.environ["LANGCHAIN_TRACING_V2"] = "true"
        os.environ["LANGCHAIN_PROJECT"] = LANGSMITH_CONFIG["project_name"]
        
        langsmith_client = Client(api_key=LANGSMITH_CONFIG["api_key"])
        LANGSMITH_AVAILABLE = True
        print("âœ… LangSmith ì¶”ì  í™œì„±í™”ë¨")
    else:
        LANGSMITH_AVAILABLE = False
        print("âš ï¸ LangSmith API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
except ImportError:
    LANGSMITH_AVAILABLE = False
    print("âš ï¸ LangSmith íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
except Exception as e:
    LANGSMITH_AVAILABLE = False
    print(f"âš ï¸ LangSmith ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

class AIService:
    def __init__(self):
        # ì´ˆê¸°í™”
        self.use_legacy = False
        self.legacy_client = None
        self.langsmith_enabled = LANGSMITH_AVAILABLE
        
        # Azure OpenAI ì„¤ì •
        try:
            if AI_CONFIG["openai_api_key"] and AI_CONFIG["openai_endpoint"]:
                # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í˜¸í™˜ì„±ì„ ìœ„í•œ ì„¤ì •
                self.client = openai.AzureOpenAI(
                    azure_endpoint=AI_CONFIG["openai_endpoint"],
                    api_key=AI_CONFIG["openai_api_key"],
                    api_version=AI_CONFIG["api_version"]
                )
                self.ai_available = True
                print("âœ… Azure OpenAI ì´ˆê¸°í™” ì„±ê³µ")
            else:
                self.client = None
                self.ai_available = False
                print("âš ï¸ Azure OpenAI ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ë”ë¯¸ ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
        except Exception as e:
            self.client = None
            self.ai_available = False
            print(f"âš ï¸ Azure OpenAI ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # í˜¸í™˜ì„± ë¬¸ì œì¸ ê²½ìš° ëŒ€ì²´ ì´ˆê¸°í™” ì‹œë„
            try:
                import openai as openai_client
                openai_client.api_type = "azure"
                openai_client.api_base = AI_CONFIG["openai_endpoint"]
                openai_client.api_version = AI_CONFIG["api_version"] 
                openai_client.api_key = AI_CONFIG["openai_api_key"]
                self.legacy_client = openai_client
                self.ai_available = True
                self.use_legacy = True
                print("âœ… ë ˆê±°ì‹œ ëª¨ë“œë¡œ Azure OpenAI ì´ˆê¸°í™” ì„±ê³µ")
            except Exception as legacy_error:
                self.use_legacy = False
                print(f"âš ï¸ ë ˆê±°ì‹œ ëª¨ë“œë„ ì‹¤íŒ¨: {legacy_error}")
        
        # Tavily ê²€ìƒ‰ í´ë¼ì´ì–¸íŠ¸
        try:
            if TAVILY_CONFIG["api_key"]:
                self.tavily_client = TavilyClient(api_key=TAVILY_CONFIG["api_key"])
                self.search_available = True
            else:
                self.tavily_client = None
                self.search_available = False
                print("âš ï¸ Tavily API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            self.tavily_client = None
            self.search_available = False
            print(f"âš ï¸ Tavily ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì¤‘ë³µ API í˜¸ì¶œ ë°©ì§€)
        self._cache = {}
        self._cache_ttl = 300  # 5ë¶„ ìºì‹œ
        
        # Azure Search í´ë¼ì´ì–¸íŠ¸
        try:
            if AZURE_SEARCH_CONFIG["endpoint"] and AZURE_SEARCH_CONFIG["admin_key"]:
                import requests
                self.azure_search_available = True
                self.azure_search_endpoint = AZURE_SEARCH_CONFIG["endpoint"]
                self.azure_search_key = AZURE_SEARCH_CONFIG["admin_key"]
                self.azure_search_index = AZURE_SEARCH_CONFIG["index_name"]
                self.azure_search_api_version = AZURE_SEARCH_CONFIG["api_version"]
                print("âœ… Azure Search ì´ˆê¸°í™” ì„±ê³µ")
            else:
                self.azure_search_available = False
                print("âš ï¸ Azure Search ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. ë¡œì»¬ ê²€ìƒ‰ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        except Exception as e:
            self.azure_search_available = False
            print(f"âš ï¸ Azure Search ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _get_cache_key(self, method: str, query: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"{method}:{hash(query)}"
    
    def _get_cached_result(self, cache_key: str):
        """ìºì‹œëœ ê²°ê³¼ ì¡°íšŒ"""
        if cache_key in self._cache:
            timestamp, result = self._cache[cache_key]
            if time.time() - timestamp < self._cache_ttl:
                return result
            else:
                # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                del self._cache[cache_key]
        return None
    
    def _set_cache(self, cache_key: str, result):
        """ê²°ê³¼ ìºì‹œ ì €ì¥"""
        self._cache[cache_key] = (time.time(), result)
    
    def test_ai_connection(self) -> Dict[str, Any]:
        """AI ì—°ê²° ìƒíƒœ í…ŒìŠ¤íŠ¸"""
        result = {
            "ai_available": self.ai_available,
            "search_available": self.search_available,
            "langsmith_enabled": self.langsmith_enabled,
            "langsmith_project": LANGSMITH_CONFIG.get("project_name", "ì—†ìŒ"),
            "endpoint": AI_CONFIG.get("openai_endpoint", "ì—†ìŒ"),
            "model": AI_CONFIG.get("deployment_name", "ì—†ìŒ"),
            "api_key_set": bool(AI_CONFIG.get("openai_api_key")),
            "tavily_key_set": bool(TAVILY_CONFIG.get("api_key")),
            "langsmith_key_set": bool(LANGSMITH_CONFIG.get("api_key"))
        }
        
        if self.ai_available:
            try:
                print("ğŸ” OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
                
                if hasattr(self, 'use_legacy') and self.use_legacy:
                    # ë ˆê±°ì‹œ ëª¨ë“œ í˜¸ì¶œ
                    response = self.legacy_client.ChatCompletion.create(
                        engine=AI_CONFIG["deployment_name"],
                        messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”."}],
                        max_tokens=50,
                        temperature=0.1
                    )
                    result["test_response"] = response.choices[0].message.content
                    result["mode"] = "legacy"
                else:
                    # ìƒˆë¡œìš´ ëª¨ë“œ í˜¸ì¶œ
                    response = self.client.chat.completions.create(
                        model=AI_CONFIG["deployment_name"],
                        messages=[{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ì¸ì‚¬í•´ì£¼ì„¸ìš”."}],
                        max_tokens=50,
                        temperature=0.1
                    )
                    result["test_response"] = response.choices[0].message.content
                    result["mode"] = "modern"
                    
                result["connection_test"] = "ì„±ê³µ"
                print("âœ… OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            except Exception as e:
                result["connection_test"] = f"ì‹¤íŒ¨: {str(e)}"
                result["test_response"] = None
                print(f"âŒ OpenAI ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        else:
            result["connection_test"] = "AI ì‚¬ìš© ë¶ˆê°€ëŠ¥"
            result["test_response"] = None
            
        return result
    
    def get_smart_analysis_prompt(self, text: str, user_type: str = "planner") -> str:
        """ì‚¬ìš©ì ìœ í˜•ì— ë§ëŠ” ìŠ¤ë§ˆíŠ¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        prompts = {
            "planner": """ë‹¹ì‹ ì€ ê²½í—˜ì´ í’ë¶€í•œ ê¸°íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ê¸°íšì ê´€ì ì—ì„œ ë‹¤ìŒì„ ì œê³µí•˜ì„¸ìš”:
            
            1. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (3-5ê°œ ë¶ˆë¦¿í¬ì¸íŠ¸)
            2. ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ (ìš°ì„ ìˆœìœ„ë³„)
            3. ì ì¬ì  ìœ„í—˜ìš”ì†Œì™€ ëŒ€ì•ˆ
            4. ì´í•´ê´€ê³„ìë³„ ê³ ë ¤ì‚¬í•­
            5. ì„±ê³µ ì§€í‘œ ì œì•ˆ
            
            ê²°ê³¼ëŠ” ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”.""",
            
            "reporter": """ë‹¹ì‹ ì€ ì „ë¬¸ ë³´ê³ ì„œ ì‘ì„±ìì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë³´ê³ ì„œ ì‘ì„±ì ê´€ì ì—ì„œ ë‹¤ìŒì„ ì œê³µí•˜ì„¸ìš”:
            
            1. í•µì‹¬ ë©”ì‹œì§€ ìš”ì•½ (í•œ ë¬¸ë‹¨)
            2. ì£¼ìš” ë°ì´í„° í¬ì¸íŠ¸ì™€ í†µê³„
            3. ë…¼ë¦¬ì  êµ¬ì¡° ì œì•ˆ (ì„œë¡ -ë³¸ë¡ -ê²°ë¡ )
            4. ì‹œê°í™” ì œì•ˆ (ì°¨íŠ¸, ê·¸ë˜í”„, í‘œ)
            5. ì¶”ê°€ í•„ìš” ì •ë³´ ëª©ë¡
            
            ê²°ê³¼ëŠ” ì„¤ë“ë ¥ ìˆê³  ë…¼ë¦¬ì ì¸ ë³´ê³ ì„œ í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”.""",
            
            "general": """ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ì œê³µí•˜ì„¸ìš”:
            
            1. ì£¼ìš” ì£¼ì œì™€ í‚¤ì›Œë“œ
            2. í•µì‹¬ ë‚´ìš© ìš”ì•½
            3. ë…¼ë¦¬ì  íë¦„ ë¶„ì„
            4. ê°œì„  ì œì•ˆ
            5. ê´€ë ¨ ìë£Œ ê²€ìƒ‰ í‚¤ì›Œë“œ
            
            ê²°ê³¼ëŠ” ëª…í™•í•˜ê³  í™œìš©í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì œê³µí•˜ì„¸ìš”."""
        }
        
        return prompts.get(user_type, prompts["general"])
    
    def analyze_text(self, text: str, user_type: str = "general") -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë¶„ì„ - ì‚¬ìš©ì ìœ í˜•ë³„ ë§ì¶¤ ë¶„ì„"""
        # LangSmith ì¶”ì  ì‹œì‘
        if self.langsmith_enabled:
            return self._analyze_text_with_langsmith(text, user_type)
        else:
            return self._analyze_text_core(text, user_type)
    
    def _analyze_text_with_langsmith(self, text: str, user_type: str = "general") -> Dict[str, Any]:
        """LangSmith ì¶”ì ê³¼ í•¨ê»˜ í…ìŠ¤íŠ¸ ë¶„ì„ - LangChain í†µí•©"""
        try:
            from langchain_openai import AzureChatOpenAI
            
            print("ğŸ” LangChain + LangSmith ì¶”ì  ì‹œì‘...")
            
            # LangChain OpenAI í´ë¼ì´ì–¸íŠ¸ (ìë™ LangSmith ì¶”ì )
            llm = AzureChatOpenAI(
                azure_endpoint=AI_CONFIG["openai_endpoint"],
                api_key=AI_CONFIG["openai_api_key"],
                api_version=AI_CONFIG["api_version"],
                deployment_name=AI_CONFIG["deployment_name"],
                temperature=AI_CONFIG["temperature"],
                max_tokens=AI_CONFIG["max_tokens"],
                model_kwargs={
                    "metadata": {
                        "user_type": user_type,
                        "text_length": len(text),
                        "project": "AI-Document-Assistant"
                    }
                }
            )
            
            # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
            system_prompt = self.get_smart_analysis_prompt(text, user_type)
            
            # LangChain invoke í˜¸ì¶œ (ìë™ìœ¼ë¡œ LangSmithì— ì¶”ì ë¨)
            messages = [
                ("system", system_prompt),
                ("human", f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{text}")
            ]
            
            print("ï¿½ LangChain Azure OpenAI API í˜¸ì¶œ ì‹œì‘...")
            start_time = time.time()
            
            response = llm.invoke(messages)
            
            end_time = time.time()
            duration = end_time - start_time
            
            print("âœ… LangChain Azure OpenAI API í˜¸ì¶œ ì„±ê³µ!")
            print(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response.content)} ë¬¸ì")
            print(f"â±ï¸ í˜¸ì¶œ ì‹œê°„: {duration:.2f}ì´ˆ")
            print("âœ… LangSmith ìë™ ì¶”ì  ì™„ë£Œ - https://smith.langchain.comì—ì„œ í™•ì¸ ê°€ëŠ¥")
            
            # ì‘ë‹µ ë‚´ìš©
            content = response.content
            
            # êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ìƒì„±
            analysis_result = self._parse_analysis_response(content, user_type)
            
            return {
                "keywords": self._extract_keywords(text),
                "topic": self._extract_topic(content),
                "summary": self._extract_summary(content),
                "analysis": analysis_result,
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text,
                "call_info": {
                    "duration_seconds": duration,
                    "response_length": len(content),
                    "status": "success",
                    "method": "langchain_azure_openai"
                }
            }
            
        except ImportError:
            print("âš ï¸ LangChain OpenAI íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ - ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜")
            return self._analyze_text_core(text, user_type)
        except Exception as langchain_error:
            print(f"âš ï¸ LangChain ì¶”ì  ì‹¤íŒ¨: {langchain_error}")
            print("ğŸ”„ ê¸°ë³¸ OpenAI ëª¨ë“œë¡œ ë¶„ì„ ê³„ì†...")
            return self._analyze_text_core(text, user_type)
    
    def _analyze_text_core(self, text: str, user_type: str = "general") -> Dict[str, Any]:
        """í•µì‹¬ í…ìŠ¤íŠ¸ ë¶„ì„ ë¡œì§"""
        if not text.strip():
            return {"keywords": [], "topic": "", "context": "", "summary": "", "analysis": {}}
        
        # AI ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ ì²´í¬ ë° ë¡œê·¸
        print(f"ğŸ” AI ë¶„ì„ ìš”ì²­: user_type={user_type}, text_length={len(text)}")
        print(f"ğŸ”§ AI ì‚¬ìš© ê°€ëŠ¥: {self.ai_available}")
        
        if not self.ai_available:
            print("âš ï¸ AI ì‚¬ìš© ë¶ˆê°€ëŠ¥ - ë”ë¯¸ ë°ì´í„° ë°˜í™˜")
            return self._get_dummy_analysis(text, user_type)
        
        try:
            # ì‚¬ìš©ì ë§ì¶¤í˜• í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            system_prompt = self.get_smart_analysis_prompt(text, user_type)
            
            print("ğŸš€ Azure OpenAI API í˜¸ì¶œ ì‹œì‘...")
            print(f"ğŸ“ Endpoint: {AI_CONFIG['openai_endpoint']}")
            print(f"ğŸ¤– Model: {AI_CONFIG['deployment_name']}")
            
            # API í˜¸ì¶œ ì‹œì‘ ì‹œê°„ ê¸°ë¡
            start_time = time.time()
            
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{text}"}
            ]
            
            # LangSmith ë¡œê¹…ì„ ìœ„í•œ í˜¸ì¶œ ì •ë³´
            call_info = {
                "messages": messages,
                "model": AI_CONFIG["deployment_name"],
                "max_tokens": AI_CONFIG["max_tokens"],
                "temperature": AI_CONFIG["temperature"],
                "user_type": user_type,
                "input_length": len(text)
            }
            
            if self.langsmith_enabled:
                print("ğŸ“ LangSmith í˜¸ì¶œ ì •ë³´ ê¸°ë¡ ì¤‘...")
            
            if hasattr(self, 'use_legacy') and self.use_legacy:
                # ë ˆê±°ì‹œ ëª¨ë“œ í˜¸ì¶œ
                print("ğŸ”§ ë ˆê±°ì‹œ ëª¨ë“œë¡œ API í˜¸ì¶œ")
                response = self.legacy_client.ChatCompletion.create(
                    engine=AI_CONFIG["deployment_name"],
                    messages=messages,
                    max_tokens=AI_CONFIG["max_tokens"],
                    temperature=AI_CONFIG["temperature"]
                )
            else:
                # ìƒˆë¡œìš´ ëª¨ë“œ í˜¸ì¶œ
                print("ğŸ”§ ëª¨ë˜ ëª¨ë“œë¡œ API í˜¸ì¶œ")
                response = self.client.chat.completions.create(
                    model=AI_CONFIG["deployment_name"],
                    messages=messages,
                    max_tokens=AI_CONFIG["max_tokens"],
                    temperature=AI_CONFIG["temperature"]
                )
            
            # API í˜¸ì¶œ ì¢…ë£Œ ì‹œê°„ ë° ì„±ëŠ¥ ë©”íŠ¸ë¦­
            end_time = time.time()
            duration = end_time - start_time
            
            print("âœ… Azure OpenAI API í˜¸ì¶œ ì„±ê³µ!")
            print(f"ğŸ“Š ì‘ë‹µ ê¸¸ì´: {len(response.choices[0].message.content)} ë¬¸ì")
            print(f"â±ï¸ í˜¸ì¶œ ì‹œê°„: {duration:.2f}ì´ˆ")
            
            # í† í° ì‚¬ìš©ëŸ‰ ì •ë³´ (ê°€ëŠ¥í•œ ê²½ìš°)
            if hasattr(response, 'usage') and response.usage:
                print(f"ğŸ¯ í† í° ì‚¬ìš©ëŸ‰: {response.usage.total_tokens} (ì…ë ¥: {response.usage.prompt_tokens}, ì¶œë ¥: {response.usage.completion_tokens})")
                call_info["token_usage"] = {
                    "total_tokens": response.usage.total_tokens,
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens
                }
            
            content = response.choices[0].message.content
            
            # LangSmith ì‘ë‹µ ì •ë³´ ì¶”ê°€
            if self.langsmith_enabled:
                call_info.update({
                    "response_length": len(content),
                    "duration_seconds": duration,
                    "status": "success"
                })
                print("ğŸ“ LangSmith ì‘ë‹µ ì •ë³´ ê¸°ë¡ ì™„ë£Œ")
            
            # êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ ìƒì„±
            analysis_result = self._parse_analysis_response(content, user_type)
            
            return {
                "keywords": self._extract_keywords(text),
                "topic": self._extract_topic(content),
                "summary": self._extract_summary(content),
                "analysis": analysis_result,
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text,
                "call_info": call_info if self.langsmith_enabled else None
            }
            
        except Exception as e:
            print(f"âŒ Azure OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
            
            # LangSmith ì—ëŸ¬ ì •ë³´
            if self.langsmith_enabled:
                error_info = {
                    "error": str(e),
                    "error_type": type(e).__name__,
                    "status": "failed"
                }
                print("ğŸ“ LangSmith ì—ëŸ¬ ì •ë³´ ê¸°ë¡")
            
            st.error(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._get_dummy_analysis(text, user_type)
    
    def _parse_analysis_response(self, content: str, user_type: str) -> Dict[str, Any]:
        """AI ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë¶„ì„ ê²°ê³¼ë¡œ íŒŒì‹±"""
        lines = content.split('\n')
        sections = {}
        current_section = ""
        current_content = []
        
        # ì„¹ì…˜ë³„ë¡œ ë‚´ìš© ë¶„ë¥˜
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # ë²ˆí˜¸ë‚˜ ì œëª©ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ì„¹ì…˜ ê°ì§€
            if re.match(r'^\d+\.', line) or line.startswith('#') or line.endswith(':'):
                if current_section and current_content:
                    sections[current_section] = '\n'.join(current_content)
                current_section = line
                current_content = []
            else:
                current_content.append(line)
        
        # ë§ˆì§€ë§‰ ì„¹ì…˜ ì²˜ë¦¬
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content)
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ êµ¬ì¡°í™”
        if user_type == "planner":
            return {
                "insights": sections.get("1. í•µì‹¬ ì¸ì‚¬ì´íŠ¸", ""),
                "action_items": sections.get("2. ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ", ""),
                "risks": sections.get("3. ì ì¬ì  ìœ„í—˜ìš”ì†Œì™€ ëŒ€ì•ˆ", ""),
                "stakeholders": sections.get("4. ì´í•´ê´€ê³„ìë³„ ê³ ë ¤ì‚¬í•­", ""),
                "metrics": sections.get("5. ì„±ê³µ ì§€í‘œ ì œì•ˆ", ""),
                "raw_content": content
            }
        elif user_type == "reporter":
            return {
                "key_message": sections.get("1. í•µì‹¬ ë©”ì‹œì§€ ìš”ì•½", ""),
                "data_points": sections.get("2. ì£¼ìš” ë°ì´í„° í¬ì¸íŠ¸ì™€ í†µê³„", ""),
                "structure": sections.get("3. ë…¼ë¦¬ì  êµ¬ì¡° ì œì•ˆ", ""),
                "visualization": sections.get("4. ì‹œê°í™” ì œì•ˆ", ""),
                "additional_info": sections.get("5. ì¶”ê°€ í•„ìš” ì •ë³´ ëª©ë¡", ""),
                "raw_content": content
            }
        else:
            return {
                "topics_keywords": sections.get("1. ì£¼ìš” ì£¼ì œì™€ í‚¤ì›Œë“œ", ""),
                "summary": sections.get("2. í•µì‹¬ ë‚´ìš© ìš”ì•½", ""),
                "logic_flow": sections.get("3. ë…¼ë¦¬ì  íë¦„ ë¶„ì„", ""),
                "improvements": sections.get("4. ê°œì„  ì œì•ˆ", ""),
                "search_keywords": sections.get("5. ê´€ë ¨ ìë£Œ ê²€ìƒ‰ í‚¤ì›Œë“œ", ""),
                "raw_content": content
            }
    
    def _get_dummy_analysis(self, text: str, user_type: str) -> Dict[str, Any]:
        """ë”ë¯¸ ë¶„ì„ ê²°ê³¼ (AIê°€ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•Œ)"""
        keywords = self._extract_keywords(text)
        
        if user_type == "planner":
            return {
                "keywords": keywords,
                "topic": "ê¸°íš ë¶„ì„ í•„ìš”",
                "summary": "ê¸°íšì ê´€ì ì—ì„œ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•œ ë‚´ìš©ì…ë‹ˆë‹¤.",
                "analysis": {
                    "insights": "â€¢ ì£¼ìš” ë…¼ì ë“¤ì„ ëª…í™•íˆ ì •ì˜ í•„ìš”\nâ€¢ ì´í•´ê´€ê³„ì ë¶„ì„ ìš”êµ¬\nâ€¢ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ í•„ìš”",
                    "action_items": "1. í˜„í™© ë¶„ì„ ìˆ˜í–‰\n2. ëª©í‘œ ì„¤ì •\n3. ì‹¤í–‰ ê³„íš ì‘ì„±",
                    "risks": "â€¢ ë¶ˆí™•ì‹¤í•œ ìš”ì†Œë“¤ ì¡´ì¬\nâ€¢ ë¦¬ì†ŒìŠ¤ ì œì•½ ê³ ë ¤ í•„ìš”",
                    "stakeholders": "â€¢ ë‚´ë¶€ íŒ€ì›ë“¤\nâ€¢ ê²½ì˜ì§„\nâ€¢ ì™¸ë¶€ íŒŒíŠ¸ë„ˆ",
                    "metrics": "â€¢ ì§„í–‰ë¥  ì¸¡ì •\nâ€¢ í’ˆì§ˆ ì§€í‘œ\nâ€¢ ì„±ê³¼ ì¸¡ì •",
                    "raw_content": f"[ë”ë¯¸ ëª¨ë“œ] {text[:200]}..."
                },
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text
            }
        elif user_type == "reporter":
            return {
                "keywords": keywords,
                "topic": "ë³´ê³ ì„œ ì‘ì„± ì§€ì›",
                "summary": "ë³´ê³ ì„œ ì‘ì„±ì„ ìœ„í•œ êµ¬ì¡°í™”ëœ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "analysis": {
                    "key_message": "í•µì‹¬ ë©”ì‹œì§€ë¥¼ ëª…í™•íˆ ì •ì˜í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ ì „ê°œí•´ì•¼ í•©ë‹ˆë‹¤.",
                    "data_points": "â€¢ ì •ëŸ‰ì  ë°ì´í„° ìˆ˜ì§‘ í•„ìš”\nâ€¢ ë¹„êµ ë¶„ì„ ë°ì´í„°\nâ€¢ íŠ¸ë Œë“œ ë¶„ì„",
                    "structure": "1. ì„œë¡ : ë°°ê²½ê³¼ ëª©ì \n2. ë³¸ë¡ : í˜„í™© ë¶„ì„ê³¼ í•´ê²°ë°©ì•ˆ\n3. ê²°ë¡ : ê¶Œê³ ì‚¬í•­",
                    "visualization": "â€¢ ë§‰ëŒ€ ì°¨íŠ¸: ë¹„êµ ë¶„ì„\nâ€¢ ì„  ê·¸ë˜í”„: íŠ¸ë Œë“œ\nâ€¢ íŒŒì´ ì°¨íŠ¸: ë¹„ìœ¨",
                    "additional_info": "â€¢ ì—…ê³„ ë²¤ì¹˜ë§ˆí¬\nâ€¢ ê²½ìŸì‚¬ ë¶„ì„\nâ€¢ ì‹œì¥ ë™í–¥",
                    "raw_content": f"[ë”ë¯¸ ëª¨ë“œ] {text[:200]}..."
                },
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text
            }
        else:
            return {
                "keywords": keywords,
                "topic": "í…ìŠ¤íŠ¸ ë¶„ì„",
                "summary": "í…ìŠ¤íŠ¸ ë‚´ìš©ì— ëŒ€í•œ ì¢…í•©ì ì¸ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.",
                "analysis": {
                    "topics_keywords": f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords[:5])}",
                    "summary": "ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ê³  êµ¬ì¡°í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.",
                    "logic_flow": "ë…¼ë¦¬ì  íë¦„ì„ ê°œì„ í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                    "improvements": "â€¢ ëª…í™•í•œ ì œëª© êµ¬ì„±\nâ€¢ ë‹¨ë½ë³„ ì£¼ì œ ëª…í™•í™”\nâ€¢ ê²°ë¡  ê°•í™”",
                    "search_keywords": f"ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords[:3])}",
                    "raw_content": f"[ë”ë¯¸ ëª¨ë“œ] {text[:200]}..."
                },
                "user_type": user_type,
                "original_text": text[:500] + "..." if len(text) > 500 else text
            }
    
    def search_related_documents(self, query: str, keywords: List[str] = None, user_type: str = "general") -> List[Dict[str, Any]]:
        """Tavilyë¥¼ ì‚¬ìš©í•œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ - ì‚¬ìš©ì íƒ€ì…ë³„ ë§ì¶¤ ê²€ìƒ‰"""
        if not self.search_available:
            return self._get_intelligent_dummy_results(query, keywords, user_type)
        
        try:
            # ì‚¬ìš©ì íƒ€ì…ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
            search_query = self._optimize_search_query(query, keywords, user_type)
            
            # Tavily ê²€ìƒ‰ ì‹¤í–‰ (ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€)
            print(f"ğŸ” Tavily í˜¸ì¶œ ì‹œì‘: query='{search_query}', depth={TAVILY_CONFIG['search_depth']}, max={TAVILY_CONFIG['max_results']}")
            response = self.tavily_client.search(
                query=search_query,
                search_depth=TAVILY_CONFIG["search_depth"],
                max_results=TAVILY_CONFIG["max_results"]
            )
            print(f"âœ… Tavily ì‘ë‹µ ìˆ˜ì‹ : {len(response.get('results', []))}ê°œ ê²°ê³¼")
            
            # ê²°ê³¼ ë³€í™˜ ë° í•„í„°ë§
            documents = []
            for i, result in enumerate(response.get("results", [])):
                doc = {
                    "id": i + 1,
                    "title": result.get("title", "ì œëª© ì—†ìŒ"),
                    "summary": self._generate_smart_summary(result.get("content", ""), user_type),
                    "content": result.get("content", ""),
                    "source": result.get("url", ""),
                    "relevance_score": self._calculate_relevance_score(result, query, keywords, user_type),
                    "keywords": keywords[:5] if keywords else [],
                    "user_type": user_type,
                    "document_type": self._classify_document_type(result, user_type)
                }
                documents.append(doc)
            
            # ê´€ë ¨ë„ìˆœ ì •ë ¬ ë° í•„í„°ë§
            documents.sort(key=lambda x: x["relevance_score"], reverse=True)
            return documents[:6]  # ìµœëŒ€ 6ê°œ
            
        except Exception as e:
            print(f"âŒ ì™¸ë¶€ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback
            print(f"âŒ ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            st.warning(f"ì‹¤ì‹œê°„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)} - ë¡œì»¬ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return self._get_intelligent_dummy_results(query, keywords, user_type)
    
    def _optimize_search_query(self, query: str, keywords: List[str], user_type: str) -> str:
        """ì‚¬ìš©ì íƒ€ì…ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”"""
        base_query = query
        
        # í‚¤ì›Œë“œ ì¶”ê°€
        if keywords:
            base_query += " " + " ".join(keywords[:3])
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ ê²€ìƒ‰ì–´ ì¶”ê°€
        type_keywords = {
            "planner": ["ê¸°íš", "ì „ëµ", "ê³„íš", "ë°©ì•ˆ", "strategy", "planning"],
            "reporter": ["ë³´ê³ ì„œ", "ë¶„ì„", "ë°ì´í„°", "í†µê³„", "report", "analysis"],
            "general": ["ì •ë³´", "ìë£Œ", "ê°€ì´ë“œ", "ë°©ë²•", "information"]
        }
        
        if user_type in type_keywords:
            base_query += " " + " OR ".join(type_keywords[user_type][:2])
        
        return base_query
    
    def _get_relevant_domains(self, user_type: str) -> List[str]:
        """ì‚¬ìš©ì íƒ€ì…ë³„ ê´€ë ¨ ë„ë©”ì¸ ë°˜í™˜"""
        domains = {
            "planner": ["harvard.edu", "mit.edu", "mckinsey.com", "pwc.com", "deloitte.com"],
            "reporter": ["statista.com", "bloomberg.com", "reuters.com", "economist.com"],
            "general": []  # ëª¨ë“  ë„ë©”ì¸ í—ˆìš©
        }
        return domains.get(user_type, [])
    
    def _generate_smart_summary(self, content: str, user_type: str) -> str:
        """ì‚¬ìš©ì íƒ€ì…ë³„ ìŠ¤ë§ˆíŠ¸ ìš”ì•½ ìƒì„±"""
        if not content or len(content) < 100:
            return content[:200] + "..." if len(content) > 200 else content
        
        # ê°„ë‹¨í•œ ìš”ì•½ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ AI ìš”ì•½ ì‚¬ìš© ê°€ëŠ¥)
        sentences = content.split('. ')
        
        if user_type == "planner":
            # ê¸°íšìë¥¼ ìœ„í•œ ì•¡ì…˜ ì¤‘ì‹¬ ìš”ì•½
            key_sentences = [s for s in sentences if any(word in s.lower() 
                           for word in ['ê³„íš', 'ì „ëµ', 'ë°©ë²•', 'ë°©ì•ˆ', 'ëª©í‘œ', 'strategy', 'plan'])]
        elif user_type == "reporter":
            # ë³´ê³ ì„œ ì‘ì„±ìë¥¼ ìœ„í•œ ë°ì´í„° ì¤‘ì‹¬ ìš”ì•½
            key_sentences = [s for s in sentences if any(word in s.lower() 
                           for word in ['ë°ì´í„°', 'í†µê³„', 'ê²°ê³¼', 'ë¶„ì„', 'ìˆ˜ì¹˜', 'data', 'analysis'])]
        else:
            # ì¼ë°˜ì ì¸ ìš”ì•½
            key_sentences = sentences[:3]
        
        if not key_sentences:
            key_sentences = sentences[:2]
        
        summary = '. '.join(key_sentences[:2])
        return summary[:300] + "..." if len(summary) > 300 else summary
    
    def _calculate_relevance_score(self, result: Dict, query: str, keywords: List[str], user_type: str) -> float:
        """ì‚¬ìš©ì íƒ€ì…ë³„ ê´€ë ¨ë„ ì ìˆ˜ ê³„ì‚°"""
        score = result.get("score", 0.5)
        
        title = result.get("title", "").lower()
        content = result.get("content", "").lower()
        query_lower = query.lower()
        
        # ê¸°ë³¸ ì ìˆ˜
        relevance = score
        
        # ì œëª© ë§¤ì¹­ ë³´ë„ˆìŠ¤
        if query_lower in title:
            relevance += 0.3
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        if keywords:
            keyword_matches = sum(1 for kw in keywords if kw.lower() in title or kw.lower() in content)
            relevance += (keyword_matches / len(keywords)) * 0.2
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ ë³´ë„ˆìŠ¤
        type_bonus_keywords = {
            "planner": ["ê¸°íš", "ì „ëµ", "ê³„íš", "ë°©ì•ˆ", "strategy", "planning", "roadmap"],
            "reporter": ["ë³´ê³ ì„œ", "ë¶„ì„", "ë°ì´í„°", "í†µê³„", "report", "analysis", "study"],
            "general": []
        }
        
        if user_type in type_bonus_keywords:
            bonus_matches = sum(1 for kw in type_bonus_keywords[user_type] 
                              if kw in title or kw in content)
            relevance += bonus_matches * 0.1
        
        return min(1.0, relevance)
    
    def _classify_document_type(self, result: Dict, user_type: str) -> str:
        """ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜"""
        title = result.get("title", "").lower()
        url = result.get("url", "").lower()
        
        if any(word in title for word in ["ë³´ê³ ì„œ", "report", "ë¶„ì„", "analysis"]):
            return "ë³´ê³ ì„œ"
        elif any(word in title for word in ["ê°€ì´ë“œ", "guide", "ë°©ë²•", "how-to"]):
            return "ê°€ì´ë“œ"
        elif any(word in url for word in ["pdf", "doc", "paper"]):
            return "ë¬¸ì„œ"
        elif any(word in title for word in ["ë‰´ìŠ¤", "news", "ê¸°ì‚¬"]):
            return "ë‰´ìŠ¤"
        else:
            return "ì¼ë°˜"
    
    def _get_intelligent_dummy_results(self, query: str, keywords: List[str], user_type: str) -> List[Dict[str, Any]]:
        """ì§€ëŠ¥í˜• ë”ë¯¸ ê²€ìƒ‰ ê²°ê³¼ - ì‚¬ìš©ì íƒ€ì…ë³„ ë§ì¶¤í˜•"""
        try:
            # ë¡œì»¬ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
            with open('data/sample_documents.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            documents = data.get("documents", [])
            
            # ì‚¬ìš©ì íƒ€ì…ë³„ í•„í„°ë§ ë° ì ìˆ˜ ì¡°ì •
            filtered_docs = []
            for doc in documents:
                relevance = self._calculate_local_relevance(doc, query, keywords, user_type)
                if relevance > 0.3:  # ìµœì†Œ ê´€ë ¨ë„ ì„ê³„ê°’
                    doc["relevance_score"] = relevance
                    doc["user_type"] = user_type
                    doc["document_type"] = self._classify_local_document(doc, user_type)
                    doc["summary"] = self._generate_smart_summary(doc.get("content", ""), user_type)
                    filtered_docs.append(doc)
            
            # ê´€ë ¨ë„ìˆœ ì •ë ¬
            filtered_docs.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            # ì‚¬ìš©ì íƒ€ì…ë³„ ì¶”ê°€ ë”ë¯¸ ë°ì´í„° ìƒì„±
            if len(filtered_docs) < 3:
                additional_docs = self._generate_type_specific_dummy_docs(query, keywords, user_type)
                filtered_docs.extend(additional_docs)
            
            return filtered_docs[:5]
            
        except Exception as e:
            # íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ë”ë¯¸ ë°ì´í„°
            return self._generate_type_specific_dummy_docs(query, keywords, user_type)
    
    def _calculate_local_relevance(self, doc: Dict, query: str, keywords: List[str], user_type: str) -> float:
        """ë¡œì»¬ ë¬¸ì„œì˜ ê´€ë ¨ë„ ê³„ì‚°"""
        relevance = 0.0
        
        title = doc.get("title", "").lower()
        content = doc.get("content", "").lower()
        doc_keywords = [kw.lower() for kw in doc.get("keywords", [])]
        query_lower = query.lower()
        
        # ì¿¼ë¦¬ ë§¤ì¹­
        if query_lower in title:
            relevance += 0.4
        elif query_lower in content:
            relevance += 0.2
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        if keywords:
            keyword_matches = sum(1 for kw in keywords 
                                if kw.lower() in title or kw.lower() in content or kw.lower() in doc_keywords)
            relevance += (keyword_matches / len(keywords)) * 0.3
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ ë³´ë„ˆìŠ¤
        type_keywords = {
            "planner": ["ê¸°íš", "ì „ëµ", "ê³„íš", "ë°©ì•ˆ", "í”„ë¡œì íŠ¸"],
            "reporter": ["ë³´ê³ ì„œ", "ë¶„ì„", "ë°ì´í„°", "í†µê³„", "ì—°êµ¬"],
            "general": ["ì •ë³´", "ìë£Œ", "ë‚´ìš©"]
        }
        
        if user_type in type_keywords:
            type_matches = sum(1 for kw in type_keywords[user_type] 
                             if kw in title or kw in content)
            relevance += type_matches * 0.15
        
        return min(1.0, relevance)
    
    def _classify_local_document(self, doc: Dict, user_type: str) -> str:
        """ë¡œì»¬ ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜"""
        title = doc.get("title", "").lower()
        keywords = [kw.lower() for kw in doc.get("keywords", [])]
        
        if any(word in title for word in ["ë¶„ì„", "ë³´ê³ ì„œ", "ì—°êµ¬"]):
            return "ë¶„ì„ ë³´ê³ ì„œ"
        elif any(word in keywords for word in ["ê°€ì´ë“œ", "ë°©ë²•", "íŠœí† ë¦¬ì–¼"]):
            return "ì‹¤ìš© ê°€ì´ë“œ"
        elif user_type == "planner" and any(word in title for word in ["ê¸°íš", "ì „ëµ", "ê³„íš"]):
            return "ê¸°íš ìë£Œ"
        elif user_type == "reporter" and any(word in keywords for word in ["ë°ì´í„°", "í†µê³„"]):
            return "ë°ì´í„° ìë£Œ"
        else:
            return "ì°¸ê³  ìë£Œ"
    
    def _generate_type_specific_dummy_docs(self, query: str, keywords: List[str], user_type: str) -> List[Dict[str, Any]]:
        """ì‚¬ìš©ì íƒ€ì…ë³„ íŠ¹í™” ë”ë¯¸ ë¬¸ì„œ ìƒì„±"""
        base_docs = []
        
        if user_type == "planner":
            base_docs = [
                {
                    "id": "dummy_plan_1",
                    "title": f"'{query}' ê¸°íš ì‹¤í–‰ ê°€ì´ë“œ",
                    "summary": f"{query}ì™€ ê´€ë ¨ëœ ê¸°íš ì—…ë¬´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ë‹¨ê³„ë³„ ê°€ì´ë“œì…ë‹ˆë‹¤.",
                    "content": f"ì´ ë¬¸ì„œëŠ” {query}ì— ëŒ€í•œ ê¸°íš í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¤ë£¨ë©°, í˜„í™© ë¶„ì„, ëª©í‘œ ì„¤ì •, ì‹¤í–‰ ê³„íš ìˆ˜ë¦½, ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë“±ì˜ í•µì‹¬ ìš”ì†Œë“¤ì„ í¬í•¨í•©ë‹ˆë‹¤.",
                    "source": "ê¸°íš ì „ë¬¸ê°€ DB",
                    "relevance_score": 0.9,
                    "keywords": keywords[:5] if keywords else ["ê¸°íš", "ì „ëµ", "ì‹¤í–‰"],
                    "user_type": user_type,
                    "document_type": "ê¸°íš ê°€ì´ë“œ"
                },
                {
                    "id": "dummy_plan_2", 
                    "title": f"'{query}' í”„ë¡œì íŠ¸ ê´€ë¦¬ ì „ëµ",
                    "summary": f"{query} ê´€ë ¨ í”„ë¡œì íŠ¸ì˜ ì„±ê³µì  ê´€ë¦¬ë¥¼ ìœ„í•œ ì „ëµê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ì…ë‹ˆë‹¤.",
                    "content": f"í”„ë¡œì íŠ¸ ê´€ë¦¬ ê´€ì ì—ì„œ {query}ë¥¼ ë‹¤ë£¨ëŠ” ë°©ë²•ë¡ ê³¼ ì‹¤ë¬´ì§„ë“¤ì´ ì•Œì•„ì•¼ í•  í•µì‹¬ ì‚¬í•­ë“¤ì„ ì •ë¦¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.",
                    "source": "PM ì‹¤ë¬´ ê°€ì´ë“œ",
                    "relevance_score": 0.85,
                    "keywords": keywords[:5] if keywords else ["í”„ë¡œì íŠ¸", "ê´€ë¦¬", "ì „ëµ"],
                    "user_type": user_type,
                    "document_type": "ê´€ë¦¬ ì „ëµ"
                }
            ]
        elif user_type == "reporter":
            base_docs = [
                {
                    "id": "dummy_report_1",
                    "title": f"'{query}' í˜„í™© ë¶„ì„ ë³´ê³ ì„œ",
                    "summary": f"{query}ì— ëŒ€í•œ ì¢…í•©ì ì¸ í˜„í™© ë¶„ì„ê³¼ ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                    "content": f"ì´ ë³´ê³ ì„œëŠ” {query}ì— ëŒ€í•œ ì •ëŸ‰ì  ë¶„ì„ ê²°ê³¼ì™€ ì‹œì¥ ë™í–¥, ì£¼ìš” ì§€í‘œë“¤ì„ í¬í•¨í•˜ì—¬ ì˜ì‚¬ê²°ì •ì— í•„ìš”í•œ ê·¼ê±° ìë£Œë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
                    "source": "ì‚°ì—… ë¶„ì„ ë¦¬í¬íŠ¸",
                    "relevance_score": 0.92,
                    "keywords": keywords[:5] if keywords else ["ë¶„ì„", "ë³´ê³ ì„œ", "ë°ì´í„°"],
                    "user_type": user_type,
                    "document_type": "ë¶„ì„ ë³´ê³ ì„œ"
                },
                {
                    "id": "dummy_report_2",
                    "title": f"'{query}' ë²¤ì¹˜ë§ˆí‚¹ ìŠ¤í„°ë””",
                    "summary": f"{query} ë¶„ì•¼ì˜ ì„ ë„ ê¸°ì—…ë“¤ê³¼ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì‚¬ë¡€ ì—°êµ¬ ìë£Œì…ë‹ˆë‹¤.",
                    "content": f"ì—…ê³„ ì„ ë„ ê¸°ì—…ë“¤ì˜ {query} ê´€ë ¨ ì „ëµê³¼ ì„±ê³¼ë¥¼ ë¶„ì„í•œ ë²¤ì¹˜ë§ˆí‚¹ ì—°êµ¬ë¡œ, ë¹„êµ ë¶„ì„ ë°ì´í„°ì™€ ì‹œì‚¬ì ì„ ì œê³µí•©ë‹ˆë‹¤.",
                    "source": "ë²¤ì¹˜ë§ˆí‚¹ ì—°êµ¬ì†Œ",
                    "relevance_score": 0.88,
                    "keywords": keywords[:5] if keywords else ["ë²¤ì¹˜ë§ˆí‚¹", "ì‚¬ë¡€", "ì—°êµ¬"],
                    "user_type": user_type,
                    "document_type": "ì—°êµ¬ ë³´ê³ ì„œ"
                }
            ]
        else:
            base_docs = [
                {
                    "id": "dummy_general_1",
                    "title": f"'{query}' ì¢…í•© ê°€ì´ë“œ",
                    "summary": f"{query}ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì´í•´ë¶€í„° ì‹¤ë¬´ ì ìš©ê¹Œì§€ í¬ê´„í•˜ëŠ” ì¢…í•© ìë£Œì…ë‹ˆë‹¤.",
                    "content": f"{query}ì˜ ê°œë…, ì¤‘ìš”ì„±, ì ìš© ë°©ë²• ë“±ì„ ì²´ê³„ì ìœ¼ë¡œ ì„¤ëª…í•˜ëŠ” ì¢…í•© ê°€ì´ë“œ ë¬¸ì„œì…ë‹ˆë‹¤.",
                    "source": "ì „ë¬¸ ìë£Œ DB",
                    "relevance_score": 0.8,
                    "keywords": keywords[:5] if keywords else ["ê°€ì´ë“œ", "ì •ë³´", "ìë£Œ"],
                    "user_type": user_type,
                    "document_type": "ì¢…í•© ê°€ì´ë“œ"
                }
            ]
        
        return base_docs
    
    def refine_text(self, text: str, style: str = "clear", user_type: str = "general") -> str:
        """í…ìŠ¤íŠ¸ ë‹¤ë“¬ê¸° - ì‚¬ìš©ì íƒ€ì…ë³„ ë§ì¶¤ ìŠ¤íƒ€ì¼"""
        if not self.ai_available:
            return self._get_dummy_refined_text(text, style, user_type)
        
        style_prompts = {
            "clear": "ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”. ë³µì¡í•œ ë¬¸ì¥ì€ ë‹¨ìˆœí™”í•˜ê³  ëª¨í˜¸í•œ í‘œí˜„ì€ êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
            "professional": "ì „ë¬¸ì ì´ê³  ì •í™•í•œ í‘œí˜„ìœ¼ë¡œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”. í•´ë‹¹ ë¶„ì•¼ì˜ ì ì ˆí•œ ì „ë¬¸ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”.",
            "concise": "ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ë‚´ìš©ìœ¼ë¡œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”. ë¶ˆí•„ìš”í•œ ìˆ˜ì‹ì–´ì™€ ì¤‘ë³µ í‘œí˜„ì„ ì œê±°í•´ì£¼ì„¸ìš”.",
            "persuasive": "ì„¤ë“ë ¥ ìˆê³  ì„íŒ©íŠ¸ ìˆê²Œ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”. ë…¼ë¦¬ì  ê·¼ê±°ì™€ ê°•ë ¥í•œ ë©”ì‹œì§€ë¡œ êµ¬ì„±í•´ì£¼ì„¸ìš”."
        }
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ ì¶”ê°€ ì§€ì¹¨
        user_context = {
            "planner": "ê¸°íšì ê´€ì ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ì¸ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
            "reporter": "ë³´ê³ ì„œ ì‘ì„±ì ê´€ì ì—ì„œ ê°ê´€ì ì´ê³  ë°ì´í„° ê¸°ë°˜ì˜ í‘œí˜„ìœ¼ë¡œ ìˆ˜ì •í•´ì£¼ì„¸ìš”.",
            "general": "ì¼ë°˜ ë…ìê°€ ì´í•´í•˜ê¸° ì‰½ë„ë¡ ìˆ˜ì •í•´ì£¼ì„¸ìš”."
        }
        
        try:
            system_prompt = f"""
            {style_prompts.get(style, 'ë” ì¢‹ê²Œ')} 
            {user_context.get(user_type, '')}
            ì›ë³¸ì˜ ì˜ë¯¸ì™€ í•µì‹¬ ë‚´ìš©ì€ ë°˜ë“œì‹œ ìœ ì§€í•˜ë©´ì„œ ê°œì„ í•´ì£¼ì„¸ìš”.
            """
            
            response = self.client.chat.completions.create(
                model=AI_CONFIG["deployment_name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ë“¬ì–´ì£¼ì„¸ìš”:\n\n{text}"}
                ],
                max_tokens=min(AI_CONFIG["max_tokens"], len(text.split()) * 3),
                temperature=0.3
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            st.error(f"í…ìŠ¤íŠ¸ ë‹¤ë“¬ê¸° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._get_dummy_refined_text(text, style, user_type)
    
    def _get_dummy_refined_text(self, text: str, style: str, user_type: str) -> str:
        """ë”ë¯¸ í…ìŠ¤íŠ¸ ë‹¤ë“¬ê¸° ê²°ê³¼"""
        if style == "clear":
            return f"[ëª…í™•ì„± ê°œì„ ] {text}\n\nâ†’ í•µì‹¬ ë‚´ìš©ì„ ë” ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ í‘œí˜„í–ˆìŠµë‹ˆë‹¤."
        elif style == "professional":
            return f"[ì „ë¬¸ì„± ê°•í™”] {text}\n\nâ†’ ì „ë¬¸ì ì¸ í‘œí˜„ê³¼ ì •í™•í•œ ìš©ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ë¢°ë„ë¥¼ ë†’ì˜€ìŠµë‹ˆë‹¤."
        elif style == "concise":
            return f"[ê°„ê²°ì„± ê°œì„ ] {text[:len(text)//2]}...\n\nâ†’ ë¶ˆí•„ìš”í•œ í‘œí˜„ì„ ì œê±°í•˜ê³  í•µì‹¬ë§Œ ë‚¨ê²¼ìŠµë‹ˆë‹¤."
        else:
            return f"[{style} ìŠ¤íƒ€ì¼] {text}\n\nâ†’ {style} ìŠ¤íƒ€ì¼ë¡œ ê°œì„ ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    def structure_content(self, text: str, structure_type: str = "outline", user_type: str = "general") -> str:
        """ë‚´ìš© êµ¬ì¡°í™” - ì‚¬ìš©ì íƒ€ì…ë³„ ë§ì¶¤ êµ¬ì¡°"""
        if not self.ai_available:
            return self._get_dummy_structured_content(text, structure_type, user_type)
        
        structure_prompts = {
            "outline": "ë‹¤ìŒ ë‚´ìš©ì„ ëª©ì°¨ì™€ ì†Œì œëª©ì´ ìˆëŠ” ì²´ê³„ì ì¸ ê°œìš” í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "steps": "ë‹¤ìŒ ë‚´ìš©ì„ ë‹¨ê³„ë³„ ê°€ì´ë“œ í˜•ì‹(1ë‹¨ê³„, 2ë‹¨ê³„...)ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "qa": "ë‹¤ìŒ ë‚´ìš©ì„ ì§ˆë¬¸ê³¼ ë‹µë³€(Q&A) í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "summary": "ë‹¤ìŒ ë‚´ìš©ì„ í•µì‹¬ ìš”ì•½, ìƒì„¸ ë‚´ìš©, ê²°ë¡  í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "presentation": "ë‹¤ìŒ ë‚´ìš©ì„ í”„ë ˆì  í…Œì´ì…˜ìš© ìŠ¬ë¼ì´ë“œ êµ¬ì¡°ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”."
        }
        
        # ì‚¬ìš©ì íƒ€ì…ë³„ êµ¬ì¡°í™” ì§€ì¹¨
        user_guidance = {
            "planner": "ì‹¤í–‰ ê³„íšê³¼ ì•¡ì…˜ ì•„ì´í…œì„ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "reporter": "ë°ì´í„°ì™€ ê·¼ê±°ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.",
            "general": "ë…ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ë…¼ë¦¬ì  íë¦„ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”."
        }
        
        try:
            system_prompt = f"""
            {structure_prompts.get(structure_type, "ë‹¤ìŒ ë‚´ìš©ì„ ì²´ê³„ì ìœ¼ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.")}
            {user_guidance.get(user_type, '')}
            
            êµ¬ì¡°í™” ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:
            - ë…¼ë¦¬ì  íë¦„ê³¼ ì—°ê²°ì„±
            - í•µì‹¬ ë‚´ìš©ì˜ ê°•ì¡°
            - ì½ê¸° ì‰¬ìš´ í˜•íƒœ
            """
            
            response = self.client.chat.completions.create(
                model=AI_CONFIG["deployment_name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": text}
                ],
                max_tokens=AI_CONFIG["max_tokens"],
                temperature=0.4
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            st.error(f"ë‚´ìš© êµ¬ì¡°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return self._get_dummy_structured_content(text, structure_type, user_type)
    
    def _get_dummy_structured_content(self, text: str, structure_type: str, user_type: str) -> str:
        """ë”ë¯¸ êµ¬ì¡°í™” ê²°ê³¼"""
        text_preview = text[:100] + "..." if len(text) > 100 else text
        
        if structure_type == "outline":
            return f"""# ì£¼ì œ ê°œìš”

## 1. ë°°ê²½ ë° í˜„í™©
{text_preview}

## 2. ì£¼ìš” ë‚´ìš©
- í•µì‹¬ í¬ì¸íŠ¸ 1: ì£¼ìš” ì´ìŠˆ ë¶„ì„
- í•µì‹¬ í¬ì¸íŠ¸ 2: í•´ê²° ë°©ì•ˆ ëª¨ìƒ‰  
- í•µì‹¬ í¬ì¸íŠ¸ 3: ê¸°ëŒ€ íš¨ê³¼

## 3. ê²°ë¡  ë° ì œì–¸
- ìš”ì•½ ì •ë¦¬
- í–¥í›„ ê³„íš
- ê¶Œê³ ì‚¬í•­

[ë”ë¯¸ ëª¨ë“œë¡œ ìƒì„±ëœ êµ¬ì¡°ì…ë‹ˆë‹¤]"""
        
        elif structure_type == "steps":
            return f"""# ë‹¨ê³„ë³„ ì‹¤í–‰ ê°€ì´ë“œ

**1ë‹¨ê³„: í˜„í™© íŒŒì•…**
- {text_preview}
- ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„

**2ë‹¨ê³„: ê³„íš ìˆ˜ë¦½**
- ëª©í‘œ ì„¤ì • ë° ìš°ì„ ìˆœìœ„ ê²°ì •
- ë¦¬ì†ŒìŠ¤ ë° ì¼ì • ê³„íš

**3ë‹¨ê³„: ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§**
- ë‹¨ê³„ë³„ ì‹¤í–‰
- ì§„í–‰ ìƒí™© ì ê²€

**4ë‹¨ê³„: í‰ê°€ ë° ê°œì„ **
- ê²°ê³¼ ë¶„ì„
- ê°œì„  ë°©ì•ˆ ë„ì¶œ

[ë”ë¯¸ ëª¨ë“œë¡œ ìƒì„±ëœ ë‹¨ê³„ì…ë‹ˆë‹¤]"""
        
        elif structure_type == "qa":
            return f"""# Q&A í˜•ì‹ ì •ë¦¬

**Q1: í•µì‹¬ ì´ìŠˆëŠ” ë¬´ì—‡ì¸ê°€?**
A: {text_preview}

**Q2: ì–´ë–¤ ì ‘ê·¼ ë°©ë²•ì´ í•„ìš”í•œê°€?**
A: ì²´ê³„ì ì¸ ë¶„ì„ê³¼ ë‹¨ê³„ë³„ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.

**Q3: ê¸°ëŒ€ë˜ëŠ” ê²°ê³¼ëŠ”?**
A: íš¨ê³¼ì ì¸ ë¬¸ì œ í•´ê²°ê³¼ ì„±ê³¼ ê°œì„ ì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Q4: ì£¼ì˜ì‚¬í•­ì€?**
A: ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ í”¼ë“œë°±ì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

[ë”ë¯¸ ëª¨ë“œë¡œ ìƒì„±ëœ Q&Aì…ë‹ˆë‹¤]"""
        
        else:
            return f"""# êµ¬ì¡°í™”ëœ ë‚´ìš©

## í•µì‹¬ ìš”ì•½
{text_preview}

## ìƒì„¸ ë‚´ìš©
- ì£¼ìš” ìš”ì†Œë“¤ì˜ êµ¬ì²´ì  ì„¤ëª…
- ê´€ë ¨ ë°ì´í„° ë° ê·¼ê±°
- ì‹¤ë¬´ ì ìš© ë°©ì•ˆ

## ê²°ë¡ 
ìš”ì•½ëœ ê²°ë¡ ê³¼ í–¥í›„ ë°©í–¥ì„±

[ë”ë¯¸ ëª¨ë“œë¡œ ìƒì„±ëœ êµ¬ì¡°ì…ë‹ˆë‹¤]"""
    
    def enhance_user_prompt(self, user_input: str) -> str:
        """1ë‹¨ê³„: ì‚¬ìš©ì ì…ë ¥ì„ AIê°€ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸ ì¬ìƒì„± (ìºì‹œ ì ìš©)"""
        
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key("enhance_prompt", user_input)
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            print(f"ğŸ’¾ ìºì‹œëœ í”„ë¡¬í”„íŠ¸ ì¬ìƒì„± ê²°ê³¼ ì‚¬ìš©")
            return cached_result
            
        if not self.ai_available:
            fallback = f"[í”„ë¡¬í”„íŠ¸ ìµœì í™”] {user_input}"
            self._set_cache(cache_key, fallback)
            return fallback
        
        try:
            system_prompt = """
            ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ AIê°€ ë” íš¨ê³¼ì ìœ¼ë¡œ ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ 
            í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            
            ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë¶„ì„í•˜ì—¬:
            1. í•µì‹¬ ì˜ë„ì™€ ëª©ì ì„ íŒŒì•…í•˜ì„¸ìš”
            2. êµ¬ì²´ì ì¸ ì§ˆë¬¸ì´ë‚˜ ìš”êµ¬ì‚¬í•­ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”  
            3. ê²€ìƒ‰ì— ìœ ìš©í•œ í‚¤ì›Œë“œë“¤ì„ í¬í•¨í•˜ì„¸ìš”
            4. ëª…í™•í•˜ê³  êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”
            
            ê²°ê³¼ëŠ” AIê°€ ë” ì •í™•í•œ ë¶„ì„ì„ í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ì´ê³  ëª…í™•í•´ì•¼ í•©ë‹ˆë‹¤.
            """
            
            response = self.client.chat.completions.create(
                model=AI_CONFIG["deployment_name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ë‹¤ìŒ ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ì—¬ AIê°€ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì¸ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ìƒì„±í•´ì£¼ì„¸ìš”:\n\n{user_input}"}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            enhanced_prompt = response.choices[0].message.content.strip()
            
            # ê²°ê³¼ ìºì‹œ ì €ì¥
            self._set_cache(cache_key, enhanced_prompt)
            return enhanced_prompt
            
        except Exception as e:
            print(f"í”„ë¡¬í”„íŠ¸ ì¬ìƒì„± ì‹¤íŒ¨: {e}")
            fallback = f"[ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸] {user_input} - êµ¬ì²´ì ì¸ ë¶„ì„ ìš”ì²­"
            self._set_cache(cache_key, fallback)
            return fallback
    
    def search_internal_documents(self, enhanced_prompt: str) -> List[Dict[str, Any]]:
        """2-1ë‹¨ê³„: ì‚¬ë‚´ ë¬¸ì„œ RAG ê²€ìƒ‰ (Azure AI Search) - ìºì‹œ ì ìš©"""
        
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key("internal_search", enhanced_prompt)
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            print(f"ğŸ’¾ ìºì‹œëœ ì‚¬ë‚´ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result
            
        try:
            if self.azure_search_available:
                # ì‹¤ì œ Azure Search API í˜¸ì¶œ
                result = self._search_azure_search_api(enhanced_prompt)
                self._set_cache(cache_key, result)
                return result
            else:
                # Azure Search ì‚¬ìš© ë¶ˆê°€ëŠ¥ì‹œ ë¡œì»¬ ê²€ìƒ‰
                result = self._search_local_documents(enhanced_prompt)
                self._set_cache(cache_key, result)
                return result
            
        except Exception as e:
            print(f"ì‚¬ë‚´ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            fallback = self._get_dummy_internal_results(enhanced_prompt)
            self._set_cache(cache_key, fallback)
            return fallback
    
    def _search_azure_search_api(self, query: str) -> List[Dict[str, Any]]:
        """ì‹¤ì œ Azure Search API ê²€ìƒ‰"""
        import requests
        
        try:
            # Azure Search REST API í˜¸ì¶œ
            search_url = f"{self.azure_search_endpoint}/indexes/{self.azure_search_index}/docs/search"
            
            headers = {
                'Content-Type': 'application/json',
                'api-key': self.azure_search_key
            }
            
            # ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_body = {
                "search": query,
                "searchMode": "any",
                "queryType": "simple",
                "select": "*",  # ëª¨ë“  í•„ë“œ ì„ íƒ (ì‹¤ì œë¡œëŠ” í•„ìš”í•œ í•„ë“œë§Œ ì„ íƒ)
                "top": 10,  # ìµœëŒ€ 10ê°œ ê²°ê³¼
                "highlight": "content",  # ë‚´ìš© í•˜ì´ë¼ì´íŠ¸
                "count": True
            }
            
            params = {
                'api-version': self.azure_search_api_version
            }
            
            print(f"ğŸ” Azure Search API í˜¸ì¶œ: {query}")
            response = requests.post(search_url, headers=headers, json=search_body, params=params)
            
            if response.status_code == 200:
                results = response.json()
                documents = results.get('value', [])
                total_count = results.get('@odata.count', 0)
                
                print(f"âœ… Azure Search ê²°ê³¼: {len(documents)}ê°œ (ì´ {total_count}ê°œ)")
                
                # ê²°ê³¼ ë³€í™˜
                converted_docs = []
                for i, doc in enumerate(documents):
                    converted_doc = {
                        "id": doc.get("id", f"azure_doc_{i}"),
                        "title": doc.get("title", doc.get("Title", "ì œëª© ì—†ìŒ")),
                        "content": doc.get("content", doc.get("Content", "")),
                        "summary": self._extract_summary_from_azure_doc(doc),
                        "source_detail": f"Azure AI Search - {self.azure_search_index}",
                        "relevance_score": doc.get("@search.score", 1.0) / 10.0,  # ì ìˆ˜ ì •ê·œí™”
                        "search_type": "azure_search",
                        "keywords": self._extract_keywords_from_azure_doc(doc),
                        "highlights": doc.get("@search.highlights", {})
                    }
                    converted_docs.append(converted_doc)
                
                return converted_docs[:5]  # ìƒìœ„ 5ê°œ
                
            else:
                print(f"âŒ Azure Search API ì˜¤ë¥˜: {response.status_code} - {response.text}")
                return self._search_local_documents(query)
                
        except Exception as e:
            print(f"âŒ Azure Search API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return self._search_local_documents(query)
    
    def _extract_summary_from_azure_doc(self, doc: Dict) -> str:
        """Azure Search ë¬¸ì„œì—ì„œ ìš”ì•½ ì¶”ì¶œ"""
        # ë‹¤ì–‘í•œ í•„ë“œëª… ì‹œë„
        summary_fields = ["summary", "Summary", "description", "Description", "abstract", "Abstract"]
        
        for field in summary_fields:
            if field in doc and doc[field]:
                return doc[field][:300] + "..." if len(doc[field]) > 300 else doc[field]
        
        # ìš”ì•½ í•„ë“œê°€ ì—†ìœ¼ë©´ contentì—ì„œ ì²« ë¶€ë¶„ ì¶”ì¶œ
        content = doc.get("content", doc.get("Content", ""))
        if content:
            sentences = content.split('.')[:3]  # ì²˜ìŒ 3ë¬¸ì¥
            return '. '.join(sentences) + "..." if len(sentences) >= 3 else content[:200] + "..."
        
        return "ìš”ì•½ ì—†ìŒ"
    
    def _extract_keywords_from_azure_doc(self, doc: Dict) -> List[str]:
        """Azure Search ë¬¸ì„œì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # í‚¤ì›Œë“œ í•„ë“œê°€ ìˆìœ¼ë©´ ì‚¬ìš©
        keyword_fields = ["keywords", "Keywords", "tags", "Tags", "categories", "Categories"]
        
        for field in keyword_fields:
            if field in doc and doc[field]:
                if isinstance(doc[field], list):
                    return doc[field][:5]
                elif isinstance(doc[field], str):
                    return doc[field].split(',')[:5]
        
        # í‚¤ì›Œë“œ í•„ë“œê°€ ì—†ìœ¼ë©´ ì œëª©ê³¼ ë‚´ìš©ì—ì„œ ì¶”ì¶œ
        text = (doc.get("title", "") + " " + doc.get("content", "")).lower()
        return self._extract_keywords(text)[:5]
    
    def _search_local_documents(self, enhanced_prompt: str) -> List[Dict[str, Any]]:
        """ë¡œì»¬ ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ (Azure Search ëŒ€ì²´)"""
        try:
            # ë¡œì»¬ ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            with open('data/sample_documents.json', 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            documents = data.get("documents", [])
            
            # ê²€ìƒ‰ì–´ ê¸°ë°˜ ê´€ë ¨ë„ ê³„ì‚°
            search_terms = enhanced_prompt.lower().split()
            relevant_docs = []
            
            for doc in documents:
                relevance_score = 0
                content = (doc.get("title", "") + " " + doc.get("content", "") + " " + " ".join(doc.get("keywords", []))).lower()
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ìœ¼ë¡œ ê´€ë ¨ë„ ê³„ì‚°
                for term in search_terms:
                    if term in content:
                        relevance_score += content.count(term) * 0.1
                
                if relevance_score > 0.3:
                    doc_copy = doc.copy()
                    doc_copy["relevance_score"] = min(1.0, relevance_score)
                    doc_copy["search_type"] = "local_search"
                    doc_copy["source_detail"] = f"ë¡œì»¬ ë¬¸ì„œ DB - {doc.get('source', 'Unknown')}"
                    relevant_docs.append(doc_copy)
            
            # ê´€ë ¨ë„ìˆœ ì •ë ¬
            relevant_docs.sort(key=lambda x: x["relevance_score"], reverse=True)
            print(f"ğŸ“ ë¡œì»¬ ê²€ìƒ‰ ê²°ê³¼: {len(relevant_docs)}ê°œ")
            return relevant_docs[:5]  # ìƒìœ„ 5ê°œ
            
        except Exception as e:
            print(f"ë¡œì»¬ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self._get_dummy_internal_results(enhanced_prompt)
    
    def _get_dummy_internal_results(self, enhanced_prompt: str) -> List[Dict[str, Any]]:
        """ë”ë¯¸ ì‚¬ë‚´ ë¬¸ì„œ ê²°ê³¼"""
        return [
            {
                "id": "internal_1",
                "title": "ì‚¬ë‚´ ì •ì±… ë¬¸ì„œ - ê´€ë ¨ ê°€ì´ë“œë¼ì¸",
                "summary": f"'{enhanced_prompt[:30]}...'ì™€ ê´€ë ¨ëœ ì‚¬ë‚´ ì •ì±… ë° ê°€ì´ë“œë¼ì¸ì…ë‹ˆë‹¤.",
                "content": f"ì‚¬ë‚´ì—ì„œ {enhanced_prompt[:50]}...ì— ëŒ€í•œ í‘œì¤€ ì ˆì°¨ì™€ ë°©ë²•ë¡ ì„ ì •ì˜í•œ ë¬¸ì„œì…ë‹ˆë‹¤.",
                "source_detail": "ì‚¬ë‚´ ë¬¸ì„œ ê´€ë¦¬ ì‹œìŠ¤í…œ (ë”ë¯¸)",
                "relevance_score": 0.9,
                "search_type": "dummy",
                "keywords": ["ì •ì±…", "ê°€ì´ë“œë¼ì¸", "ì ˆì°¨"]
            }
        ]
    
    def search_external_references(self, enhanced_prompt: str) -> List[Dict[str, Any]]:
        """2-2ë‹¨ê³„: ì‚¬ì™¸ ì¸í„°ë„· ê²€ìƒ‰ (Tavily) - ìœ ì‚¬ì‚¬ë¡€, ë ˆí¼ëŸ°ìŠ¤ (ìºì‹œ ì ìš©)"""
        
        # ìºì‹œ í™•ì¸
        cache_key = self._get_cache_key("external_search", enhanced_prompt)
        cached_result = self._get_cached_result(cache_key)
        if cached_result:
            print(f"ğŸ’¾ ìºì‹œëœ ì™¸ë¶€ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
            return cached_result
            
        try:
            if self.search_available and self.tavily_client:
                # Tavilyë¡œ ì‹¤ì œ ì›¹ ê²€ìƒ‰ (ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€)
                # enhanced_promptê°€ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 100ìë§Œ ì‚¬ìš©
                short_prompt = enhanced_prompt[:100] if len(enhanced_prompt) > 100 else enhanced_prompt
                search_query = f"{short_prompt} ì‚¬ë¡€ ë ˆí¼ëŸ°ìŠ¤ best practice"
                print(f"ğŸ” Tavily ì™¸ë¶€ê²€ìƒ‰ í˜¸ì¶œ: query='{search_query}', depth=advanced, max=6")
                response = self.tavily_client.search(
                    query=search_query,
                    search_depth="advanced",
                    max_results=6
                )
                print(f"âœ… Tavily ì™¸ë¶€ê²€ìƒ‰ ì‘ë‹µ: {len(response.get('results', []))}ê°œ ê²°ê³¼")
                
                external_docs = []
                for i, result in enumerate(response.get("results", [])):
                    doc = {
                        "id": f"external_{i+1}",
                        "title": result.get("title", "ì œëª© ì—†ìŒ"),
                        "summary": result.get("content", "")[:300] + "...",
                        "content": result.get("content", ""),
                        "source_detail": f"ì™¸ë¶€ ì°¸ì¡° - {result.get('url', '')}",
                        "url": result.get("url", ""),
                        "relevance_score": result.get("score", 0.5),
                        "search_type": "external",
                        "keywords": self._extract_keywords(result.get("content", ""))[:5]
                    }
                    external_docs.append(doc)
                
                # ê²°ê³¼ ìºì‹œ ì €ì¥
                self._set_cache(cache_key, external_docs)
                return external_docs
                
            else:
                # Tavily ì‚¬ìš© ë¶ˆê°€ëŠ¥ì‹œ ë”ë¯¸ ë°ì´í„°
                fallback = self._get_dummy_external_results(enhanced_prompt)
                self._set_cache(cache_key, fallback)
                return fallback
                
        except Exception as e:
            print(f"âŒ ì™¸ë¶€ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            print(f"âŒ ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback
            print(f"âŒ ìƒì„¸ ì˜¤ë¥˜:\n{traceback.format_exc()}")
            fallback = self._get_dummy_external_results(enhanced_prompt)
            self._set_cache(cache_key, fallback)
            return fallback
    
    def _get_dummy_external_results(self, enhanced_prompt: str) -> List[Dict[str, Any]]:
        """ì™¸ë¶€ ê²€ìƒ‰ ë”ë¯¸ ê²°ê³¼"""
        return [
            {
                "id": "external_1",
                "title": f"Best Practices for {enhanced_prompt[:30]}... - Industry Report",
                "summary": f"ì—…ê³„ì—ì„œ ê²€ì¦ëœ {enhanced_prompt[:20]}...ì— ëŒ€í•œ ëª¨ë²” ì‚¬ë¡€ì™€ ì„±ê³µ ì‚¬ë¡€ë“¤ì„ ì •ë¦¬í•œ ë³´ê³ ì„œì…ë‹ˆë‹¤.",
                "content": f"ë‹¤ì–‘í•œ ê¸°ì—…ë“¤ì´ {enhanced_prompt[:30]}... ê´€ë ¨ í”„ë¡œì íŠ¸ì—ì„œ ì–»ì€ ì¸ì‚¬ì´íŠ¸ì™€ êµí›ˆë“¤ì„ ì¢…í•©í•œ ìë£Œì…ë‹ˆë‹¤.",
                "source_detail": "ì™¸ë¶€ ì°¸ì¡° - Industry Research Portal",
                "url": "https://example.com/best-practices",
                "relevance_score": 0.85,
                "search_type": "external",
                "keywords": ["best practice", "industry", "case study"]
            },
            {
                "id": "external_2", 
                "title": f"Case Study: Successful Implementation of {enhanced_prompt[:25]}...",
                "summary": f"ì‹¤ì œ ê¸°ì—…ì—ì„œ {enhanced_prompt[:20]}...ë¥¼ ì„±ê³µì ìœ¼ë¡œ êµ¬í˜„í•œ ì‚¬ë¡€ ì—°êµ¬ì…ë‹ˆë‹¤.",
                "content": f"Aì‚¬ì—ì„œ {enhanced_prompt[:30]}... í”„ë¡œì íŠ¸ë¥¼ í†µí•´ ì–»ì€ ì„±ê³¼ì™€ êµ¬í˜„ ê³¼ì •ì˜ ì„¸ë¶€ì‚¬í•­ì„ ë‹¤ë£¹ë‹ˆë‹¤.",
                "source_detail": "ì™¸ë¶€ ì°¸ì¡° - Business Case Studies",
                "url": "https://example.com/case-study",
                "relevance_score": 0.82,
                "search_type": "external", 
                "keywords": ["case study", "implementation", "success"]
            }
        ]
    
    def generate_optimized_analysis(self, enhanced_prompt: str, internal_docs: List[Dict], external_docs: List[Dict], original_input: str) -> Dict[str, Any]:
        """3ë‹¨ê³„: ìµœì í™”ëœ í†µí•© ë¶„ì„ ê²°ê³¼ ìƒì„± (ë‹¨ì¼ API í˜¸ì¶œ)"""
        try:
            if not self.ai_available:
                return self._get_fallback_analysis(enhanced_prompt, internal_docs, external_docs)
            
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
            internal_summary = self._summarize_search_results(internal_docs, "ì‚¬ë‚´ ë¬¸ì„œ")
            external_summary = self._summarize_search_results(external_docs, "ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤")
            
            # í†µí•© ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³ , ì œê³µëœ ì‚¬ë‚´ ë¬¸ì„œì™€ ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì¢…í•©í•˜ì—¬ 
ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ë¶„ì„ ê²°ê³¼ëŠ” ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ìš”ì•½ ë° í•µì‹¬ ì¸ì‚¬ì´íŠ¸
2. ì‚¬ë‚´ ê´€ì  ë¶„ì„
3. ì™¸ë¶€ ë²¤ì¹˜ë§ˆí‚¹ ë¶„ì„
4. í†µí•© ê¶Œê³ ì‚¬í•­
5. êµ¬ì²´ì  ì‹¤í–‰ ë°©ì•ˆ"""

            user_prompt = f"""
## ë¶„ì„ ìš”ì²­
**ì›ë³¸ ì…ë ¥:** {original_input}
**ìµœì í™”ëœ ë¶„ì„ ë²”ìœ„:** {enhanced_prompt}

## ê²€ìƒ‰ ê²°ê³¼
### ì‚¬ë‚´ ë¬¸ì„œ ë¶„ì„
{internal_summary}

### ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤ ë¶„ì„
{external_summary}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì´ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""

            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            analysis_content = response.choices[0].message.content
            
            return {
                "title": "ğŸ¯ í†µí•© AI ë¶„ì„ ê²°ê³¼",
                "content": analysis_content,
                "internal_docs_count": len(internal_docs),
                "external_docs_count": len(external_docs),
                "confidence": 0.9,
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
        except Exception as e:
            print(f"ìµœì í™”ëœ ë¶„ì„ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._get_fallback_analysis(enhanced_prompt, internal_docs, external_docs)
    
    def _get_fallback_analysis(self, prompt: str, internal_docs: List[Dict], external_docs: List[Dict]) -> Dict[str, Any]:
        """API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ í´ë°± ë¶„ì„"""
        return {
            "title": "ğŸ“‹ ê¸°ë³¸ ë¶„ì„ ê²°ê³¼",
            "content": f"""
## ğŸ“‹ ë¶„ì„ ê²°ê³¼ (í´ë°± ëª¨ë“œ)

### ğŸ¯ ë¶„ì„ ìš”ì²­
{prompt[:200]}...

### ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
- **ì‚¬ë‚´ ë¬¸ì„œ**: {len(internal_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨
- **ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤**: {len(external_docs)}ê°œ ì°¸ì¡° ìë£Œ ë°œê²¬

### ğŸ’¡ ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸
ê²€ìƒ‰ëœ ìë£Œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤. 
AI ì„œë¹„ìŠ¤ ì—°ê²° í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.

### ğŸ” ì°¸ê³  ìë£Œ
""" + "\n".join([f"- {doc.get('title', 'N/A')}" for doc in (internal_docs + external_docs)[:5]]),
            "internal_docs_count": len(internal_docs),
            "external_docs_count": len(external_docs),
            "confidence": 0.5,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        }

    def generate_multiple_analysis_versions(self, enhanced_prompt: str, internal_docs: List[Dict], external_docs: List[Dict], original_input: str) -> List[Dict[str, Any]]:
        """3ë‹¨ê³„: ì—¬ëŸ¬ ë²„ì „ì˜ ë¶„ì„ ê²°ê³¼ ìƒì„± (ë ˆê±°ì‹œ ì§€ì›ìš©)"""
        try:
            # ìµœì í™”ëœ ë²„ì „ì„ ì‚¬ìš©í•˜ë˜, ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë˜í•‘
            optimized_result = self.generate_optimized_analysis(enhanced_prompt, internal_docs, external_docs, original_input)
            
            # ê¸°ì¡´ ë‹¤ì¤‘ ë²„ì „ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            return [{
                "title": optimized_result["title"],
                "description": "ì‚¬ë‚´ ë¬¸ì„œì™€ ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤ë¥¼ í†µí•©í•œ ìµœì í™”ëœ ë¶„ì„",
                "content": optimized_result["content"],
                "priority": 1,
                "confidence": optimized_result["confidence"]
            }]
            
        except Exception as e:
            print(f"ë‹¤ì¤‘ ë²„ì „ ìƒì„± ì‹¤íŒ¨: {e}")
            return self._get_dummy_analysis_versions(enhanced_prompt, original_input)
    
    def _summarize_search_results(self, docs: List[Dict], source_type: str) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
        if not docs:
            return f"{source_type}ì—ì„œ ê´€ë ¨ ìë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        summaries = []
        for doc in docs[:3]:  # ìƒìœ„ 3ê°œë§Œ
            title = doc.get("title", "ì œëª© ì—†ìŒ")
            summary = doc.get("summary", "")[:100]
            summaries.append(f"- {title}: {summary}...")
        
        return f"{source_type} ê²€ìƒ‰ ê²°ê³¼ ({len(docs)}ê°œ):\n" + "\n".join(summaries)
    
    def _generate_comprehensive_version(self, prompt: str, internal_summary: str, external_summary: str, original_input: str) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ë²„ì „"""
        return {
            "title": "ğŸ¯ ì¢…í•© ë¶„ì„ (ì‚¬ë‚´+ì™¸ë¶€ í†µí•©)",
            "description": "ì‚¬ë‚´ ë¬¸ì„œì™€ ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤ë¥¼ ì¢…í•©í•œ ê· í˜•ì¡íŒ ë¶„ì„",
            "content": f"""
## ğŸ¯ ì¢…í•© ë¶„ì„ ê²°ê³¼

### ğŸ“‹ ë¶„ì„ ê°œìš”
**ì›ë³¸ ìš”ì²­:** {original_input[:100]}...
**ìµœì í™”ëœ ë¶„ì„ ë²”ìœ„:** {prompt[:150]}...

### ğŸ¢ ì‚¬ë‚´ ê´€ì 
{internal_summary}

### ğŸŒ ì™¸ë¶€ ë²¤ì¹˜ë§ˆí‚¹
{external_summary}

### ğŸ’¡ í†µí•© ì¸ì‚¬ì´íŠ¸
1. **ì‚¬ë‚´-ì™¸ë¶€ gap ë¶„ì„**: í˜„ì¬ ì‚¬ë‚´ ë°©ì‹ê³¼ ì—…ê³„ ëª¨ë²”ì‚¬ë¡€ ë¹„êµ
2. **ì ìš© ê°€ëŠ¥í•œ ì™¸ë¶€ ì‚¬ë¡€**: ìš°ë¦¬ ì¡°ì§ì— ì í•©í•œ ë ˆí¼ëŸ°ìŠ¤ ì‹ë³„  
3. **ê· í˜•ì¡íŒ ì ‘ê·¼ë²•**: ë‚´ë¶€ ì—­ëŸ‰ê³¼ ì™¸ë¶€ íŠ¸ë Œë“œë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ë°©í–¥ì„±

### ğŸš€ í†µí•© ì‹¤í–‰ ë°©ì•ˆ
- ë‹¨ê¸°: ì‚¬ë‚´ ê¸°ì¤€ ë³´ì™„ + ì™¸ë¶€ ëª¨ë²”ì‚¬ë¡€ ì¼ë¶€ ë„ì…
- ì¤‘ê¸°: ì ì§„ì  ì™¸ë¶€ í‘œì¤€ ì ìš© 
- ì¥ê¸°: ì‚¬ë‚´ ê³ ìœ  ëª¨ë¸ê³¼ ì—…ê³„ í‘œì¤€ì˜ ìµœì  ì¡°í•©
""",
            "priority": 1,
            "confidence": 0.9
        }
    
    def _generate_internal_focused_version(self, prompt: str, internal_docs: List[Dict], original_input: str) -> Dict[str, Any]:
        """ì‚¬ë‚´ ì¤‘ì‹¬ ë¶„ì„ ë²„ì „"""
        return {
            "title": "ğŸ¢ ì‚¬ë‚´ ì •ì±… ì¤‘ì‹¬ ë¶„ì„", 
            "description": "ê¸°ì¡´ ì‚¬ë‚´ ë¬¸ì„œì™€ ì •ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ë¶„ì„",
            "content": f"""
## ğŸ¢ ì‚¬ë‚´ ì •ì±… ì¤‘ì‹¬ ë¶„ì„

### ğŸ“‹ ë¶„ì„ ê¸°ì¤€
ê¸°ì¡´ ì‚¬ë‚´ ë¬¸ì„œ, ì •ì±…, ê°€ì´ë“œë¼ì¸ì„ ê¸°ë°˜ìœ¼ë¡œ í˜„ì‹¤ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.

### ğŸ“š ì°¸ì¡° ì‚¬ë‚´ ìë£Œ
""" + "\n".join([f"- {doc.get('title', '')}: {doc.get('summary', '')[:100]}..." for doc in internal_docs[:3]]) + f"""

### ğŸ’¼ ì‚¬ë‚´ ê´€ì  ë¶„ì„
1. **í˜„í–‰ ì •ì±… ë¶€í•©ì„±**: ê¸°ì¡´ ì‚¬ë‚´ ê·œì •ê³¼ì˜ ì •í•©ì„± ê²€í† 
2. **ë‚´ë¶€ ë¦¬ì†ŒìŠ¤ í™œìš©**: í˜„ì¬ ê°€ìš©í•œ ì¸ë ¥, ì˜ˆì‚°, ì‹œìŠ¤í…œ ê³ ë ¤
3. **ì¡°ì§ ë¬¸í™” ì í•©ì„±**: ìš°ë¦¬ ì¡°ì§ì˜ íŠ¹ì„±ì— ë§ëŠ” ì ‘ê·¼ë²•

### ğŸ¯ ì‚¬ë‚´ ì‹¤í–‰ ë°©ì•ˆ
- **ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥**: ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ê°œì„ 
- **ë‹¨ê¸° ì ìš©**: í˜„í–‰ ì •ì±… ë²”ìœ„ ë‚´ ë³€í™”
- **ìŠ¹ì¸ í•„ìš”**: ì •ì±… ë³€ê²½ì´ í•„ìš”í•œ ì¤‘ì¥ê¸° ê³„íš
""",
            "priority": 2,
            "confidence": 0.85
        }
    
    def _generate_external_focused_version(self, prompt: str, external_docs: List[Dict], original_input: str) -> Dict[str, Any]:
        """ì™¸ë¶€ ë²¤ì¹˜ë§ˆí‚¹ ì¤‘ì‹¬ ë¶„ì„ ë²„ì „"""  
        return {
            "title": "ğŸŒ ì™¸ë¶€ ë²¤ì¹˜ë§ˆí‚¹ ë¶„ì„",
            "description": "ì—…ê³„ ëª¨ë²”ì‚¬ë¡€ì™€ ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤ ì¤‘ì‹¬ì˜ í˜ì‹ ì  ì ‘ê·¼",
            "content": f"""
## ğŸŒ ì™¸ë¶€ ë²¤ì¹˜ë§ˆí‚¹ ë¶„ì„

### ğŸ¯ ë¶„ì„ ê´€ì 
ì—…ê³„ ì„ ë„ ê¸°ì—…ë“¤ì˜ ëª¨ë²”ì‚¬ë¡€ì™€ ìµœì‹  íŠ¸ë Œë“œë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜ì‹ ì  ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.

### ğŸ“Š ì™¸ë¶€ ë ˆí¼ëŸ°ìŠ¤
""" + "\n".join([f"- {doc.get('title', '')}: {doc.get('summary', '')[:100]}..." for doc in external_docs[:3]]) + f"""

### ğŸ† ì—…ê³„ ëª¨ë²”ì‚¬ë¡€ ë¶„ì„
1. **ì„ ë„ê¸°ì—… ì‚¬ë¡€**: ì—…ê³„ 1ìœ„ ê¸°ì—…ë“¤ì˜ ì ‘ê·¼ë²• 
2. **í˜ì‹ ì  ë°©ë²•ë¡ **: ìµœì‹  íŠ¸ë Œë“œì™€ ì‹ ê¸°ìˆ  í™œìš©
3. **ì„±ê³µ ìš”ì¸**: ì™¸ë¶€ ì„±ê³µì‚¬ë¡€ì˜ í•µì‹¬ í¬ì¸íŠ¸

### ğŸš€ í˜ì‹  ì‹¤í–‰ ë°©ì•ˆ  
- **ë²¤ì¹˜ë§ˆí‚¹ í¬ì¸íŠ¸**: ì¦‰ì‹œ ì°¸ê³ í•  ìˆ˜ ìˆëŠ” ì™¸ë¶€ ì‚¬ë¡€
- **ì ìš© ë¡œë“œë§µ**: ë‹¨ê³„ì  ë„ì…ì„ ìœ„í•œ ê³„íš
- **ì°¨ë³„í™” ì „ëµ**: ìš°ë¦¬ë§Œì˜ ê³ ìœ í•œ ê²½ìŸìš°ìœ„ ì°½ì¶œ
""",
            "priority": 3,
            "confidence": 0.8
        }
    
    def _generate_action_focused_version(self, prompt: str, internal_summary: str, external_summary: str, original_input: str) -> Dict[str, Any]:
        """ì‹¤í–‰ ê³„íš ì¤‘ì‹¬ ë¶„ì„ ë²„ì „"""
        return {
            "title": "âš¡ ì‹¤í–‰ ê³„íš ì¤‘ì‹¬ ë¶„ì„",
            "description": "êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ í”Œëœ ì¤‘ì‹¬ì˜ ë¶„ì„", 
            "content": f"""
## âš¡ ì‹¤í–‰ ê³„íš ì¤‘ì‹¬ ë¶„ì„

### ğŸ¯ ì‹¤í–‰ ìš°ì„ ìˆœìœ„
ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê²ƒë¶€í„° ë‹¨ê³„ì ìœ¼ë¡œ ì¶”ì§„í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì  ë°©ì•ˆì„ ì œì‹œí•©ë‹ˆë‹¤.

### ğŸš€ ì¦‰ì‹œ ì‹¤í–‰ (1-2ì£¼)
1. **í˜„í™© íŒŒì•…**: ê´€ë ¨ ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„  
2. **íŒ€ êµ¬ì„±**: ì‹¤í–‰ ë‹´ë‹¹ì ë° ì§€ì›íŒ€ í¸ì„±
3. **ì´ˆê¸° ê³„íš**: ì„¸ë¶€ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½

### ğŸ“… ë‹¨ê¸° ì‹¤í–‰ (1-3ê°œì›”)  
1. **íŒŒì¼ëŸ¿ ìš´ì˜**: ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
2. **í”¼ë“œë°± ìˆ˜ì§‘**: ì´ˆê¸° ê²°ê³¼ ë¶„ì„ ë° ê°œì„ ì  íŒŒì•…
3. **í”„ë¡œì„¸ìŠ¤ ì •ë¦½**: í‘œì¤€ ìš´ì˜ ì ˆì°¨ í™•ë¦½

### ğŸ¯ ì¤‘ì¥ê¸° ì‹¤í–‰ (3-12ê°œì›”)
1. **ì „ë©´ ë„ì…**: ì¡°ì§ ì „ì²´ë¡œ í™•ëŒ€ ì ìš©  
2. **ì„±ê³¼ ì¸¡ì •**: KPI ê¸°ë°˜ ì„±ê³¼ í‰ê°€
3. **ì§€ì† ê°œì„ **: ì •ê¸°ì  ë¦¬ë·° ë° ì—…ë°ì´íŠ¸

### ğŸ“Š í•„ìš” ë¦¬ì†ŒìŠ¤ ë° ì˜ˆì‚°
- **ì¸ë ¥**: í”„ë¡œì íŠ¸ ë§¤ë‹ˆì € 1ëª…, ì‹¤í–‰íŒ€ 3-5ëª…
- **ì˜ˆì‚°**: ì´ˆê¸° íˆ¬ì ë° ìš´ì˜ ë¹„ìš© ê³„íš
- **ì‹œìŠ¤í…œ**: í•„ìš” ë„êµ¬ ë° ì¸í”„ë¼ êµ¬ì¶•
""",
            "priority": 4, 
            "confidence": 0.9
        }
    
    def _get_dummy_analysis_versions(self, enhanced_prompt: str, original_input: str) -> List[Dict[str, Any]]:
        """ë”ë¯¸ ë¶„ì„ ë²„ì „ë“¤"""
        return [
            {
                "title": "ğŸ¯ ì¢…í•© ë¶„ì„ (ë”ë¯¸)",
                "description": "ì‚¬ë‚´ì™¸ ìë£Œë¥¼ ì¢…í•©í•œ ê· í˜•ì¡íŒ ë¶„ì„",
                "content": f"[ë”ë¯¸ ëª¨ë“œ] {enhanced_prompt}ì— ëŒ€í•œ ì¢…í•©ì  ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.",
                "priority": 1,
                "confidence": 0.7
            },
            {
                "title": "ğŸ¢ ì‚¬ë‚´ ì •ì±… ì¤‘ì‹¬ (ë”ë¯¸)",
                "description": "ê¸°ì¡´ ì‚¬ë‚´ ê¸°ì¤€ì— ë§ì¶˜ í˜„ì‹¤ì  ë°©ì•ˆ",
                "content": f"[ë”ë¯¸ ëª¨ë“œ] ì‚¬ë‚´ ì •ì±…ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ {enhanced_prompt} ë¶„ì„ì…ë‹ˆë‹¤.",
                "priority": 2, 
                "confidence": 0.7
            }
        ]

    def generate_action_plan(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ì•¡ì…˜ í”Œëœ ìƒì„±"""
        if not self.ai_available:
            return self._get_dummy_action_plan(analysis_result)
        
        try:
            text = analysis_result.get("original_text", "")
            user_type = analysis_result.get("user_type", "general")
            analysis = analysis_result.get("analysis", {})
            
            system_prompt = f"""
            ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ í”Œëœì„ ìƒì„±í•´ì£¼ì„¸ìš”.
            
            ì‚¬ìš©ì íƒ€ì…: {user_type}
            
            ì•¡ì…˜ í”Œëœì—ëŠ” ë‹¤ìŒì„ í¬í•¨í•´ì£¼ì„¸ìš”:
            1. ì¦‰ì‹œ ì‹¤í–‰ í•­ëª© (1-3ê°œ)
            2. ë‹¨ê¸° ê³„íš (1ì£¼-1ê°œì›”)
            3. ì¤‘ì¥ê¸° ê³„íš (1-6ê°œì›”)
            4. í•„ìš” ë¦¬ì†ŒìŠ¤
            5. ì„±ê³µ ì§€í‘œ
            """
            
            response = self.client.chat.completions.create(
                model=AI_CONFIG["deployment_name"],
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ë¶„ì„ ë‚´ìš©: {text}\n\në¶„ì„ ê²°ê³¼: {analysis}"}
                ],
                max_tokens=AI_CONFIG["max_tokens"],
                temperature=0.5
            )
            
            content = response.choices[0].message.content
            
            return {
                "action_plan": content,
                "user_type": user_type,
                "generated_at": time.time()
            }
            
        except Exception as e:
            st.error(f"ì•¡ì…˜ í”Œëœ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return self._get_dummy_action_plan(analysis_result)
    
    def _get_dummy_action_plan(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """ë”ë¯¸ ì•¡ì…˜ í”Œëœ"""
        user_type = analysis_result.get("user_type", "general")
        
        if user_type == "planner":
            plan = """# ê¸°íš ì‹¤í–‰ ì•¡ì…˜ í”Œëœ

## ğŸš€ ì¦‰ì‹œ ì‹¤í–‰ í•­ëª©
1. í˜„í™© ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘
2. í•µì‹¬ ì´í•´ê´€ê³„ì ë¯¸íŒ… ì¼ì • ì¡°ìœ¨
3. í”„ë¡œì íŠ¸ ë²”ìœ„ ë° ëª©í‘œ ëª…í™•í™”

## ğŸ“… ë‹¨ê¸° ê³„íš (1ì£¼-1ê°œì›”)
- ìƒì„¸ ê¸°íšì„œ ì‘ì„±
- ì˜ˆì‚° ë° ë¦¬ì†ŒìŠ¤ ê³„íš ìˆ˜ë¦½
- íŒ€ êµ¬ì„± ë° ì—­í•  ë°°ì •

## ğŸ¯ ì¤‘ì¥ê¸° ê³„íš (1-6ê°œì›”)
- ë‹¨ê³„ë³„ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§
- ì¤‘ê°„ í‰ê°€ ë° ì¡°ì •
- ì„±ê³¼ ì¸¡ì • ë° ë³´ê³ 

## ğŸ“Š í•„ìš” ë¦¬ì†ŒìŠ¤
- ì¸ë ¥: ê¸°íšíŒ€, ì‹¤í–‰íŒ€
- ì˜ˆì‚°: í”„ë¡œì íŠ¸ ê·œëª¨ì— ë”°ë¼
- ì‹œìŠ¤í…œ: í˜‘ì—… ë„êµ¬, ë¶„ì„ ë„êµ¬

## âœ… ì„±ê³µ ì§€í‘œ
- ëª©í‘œ ë‹¬ì„±ë¥ 
- ì¼ì • ì¤€ìˆ˜ìœ¨
- í’ˆì§ˆ ë§Œì¡±ë„"""
        
        elif user_type == "reporter":
            plan = """# ë³´ê³ ì„œ ì‘ì„± ì•¡ì…˜ í”Œëœ

## ğŸš€ ì¦‰ì‹œ ì‹¤í–‰ í•­ëª©
1. ë³´ê³ ì„œ ëª©ì ê³¼ ëŒ€ìƒ ë…ì ëª…í™•í™”
2. í•„ìš” ë°ì´í„° ë° ìë£Œ ë¦¬ìŠ¤íŠ¸ ì‘ì„±
3. ë³´ê³ ì„œ êµ¬ì¡° ë° ëª©ì°¨ ì´ˆì•ˆ ì‘ì„±

## ğŸ“… ë‹¨ê¸° ê³„íš (1ì£¼-1ê°œì›”)
- ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„
- ì´ˆì•ˆ ì‘ì„± ë° ê²€í† 
- ì‹œê° ìë£Œ(ì°¨íŠ¸, ê·¸ë˜í”„) ì œì‘

## ğŸ¯ ì¤‘ì¥ê¸° ê³„íš (1-6ê°œì›”)
- ì •ê¸° ë³´ê³ ì„œ í…œí”Œë¦¿ ê°œë°œ
- ë°ì´í„° ìˆ˜ì§‘ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•
- ë³´ê³ ì„œ í’ˆì§ˆ ê°œì„  í”„ë¡œì„¸ìŠ¤ ì •ë¦½

## ğŸ“Š í•„ìš” ë¦¬ì†ŒìŠ¤
- ë°ì´í„°: ê´€ë ¨ í†µê³„, ë¶„ì„ ìë£Œ
- ë„êµ¬: ë¶„ì„ ì†Œí”„íŠ¸ì›¨ì–´, ë””ìì¸ íˆ´
- ì‹œê°„: ì¶©ë¶„í•œ ê²€í†  ë° ìˆ˜ì • ê¸°ê°„

## âœ… ì„±ê³µ ì§€í‘œ
- ë³´ê³ ì„œ ì™„ì„±ë„
- ë°ì´í„° ì •í™•ì„±
- ë…ì ë§Œì¡±ë„"""
        
        else:
            plan = """# ì¢…í•© ì•¡ì…˜ í”Œëœ

## ğŸš€ ì¦‰ì‹œ ì‹¤í–‰ í•­ëª©
1. í•µì‹¬ ì´ìŠˆ ì •ë¦¬ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •
2. ê´€ë ¨ ìë£Œ ë° ì •ë³´ ìˆ˜ì§‘
3. ì‹¤í–‰ íŒ€ êµ¬ì„± ë° ì—­í•  ë¶„ë‹´

## ğŸ“… ë‹¨ê¸° ê³„íš (1ì£¼-1ê°œì›”)
- ì„¸ë¶€ ê³„íš ìˆ˜ë¦½
- ì´ˆê¸° ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸
- í”¼ë“œë°± ìˆ˜ì§‘ ë° ê°œì„ 

## ğŸ¯ ì¤‘ì¥ê¸° ê³„íš (1-6ê°œì›”)
- ë³¸ê²©ì ì¸ ì‹¤í–‰ ë‹¨ê³„
- ì„±ê³¼ ëª¨ë‹ˆí„°ë§ ë° ì¡°ì •
- ìµœì¢… í‰ê°€ ë° ì •ë¦¬

## ğŸ“Š í•„ìš” ë¦¬ì†ŒìŠ¤
- ì¸ë ¥: ë‹¤ì–‘í•œ ë¶„ì•¼ ì „ë¬¸ê°€
- ë„êµ¬: ì—…ë¬´ íš¨ìœ¨ì„± ë„êµ¬
- ì˜ˆì‚°: í”„ë¡œì íŠ¸ ê·œëª¨ë³„ ì ì • ì˜ˆì‚°

## âœ… ì„±ê³µ ì§€í‘œ
- ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
- íš¨ìœ¨ì„± ê°œì„ ë„
- ë§Œì¡±ë„ í‰ê°€"""
        
        return {
            "action_plan": plan,
            "user_type": user_type,
            "generated_at": time.time()
        }
    
    def _extract_keywords(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜"""
        if not text.strip():
            return []
        
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (í•œêµ­ì–´)
        stop_words = {
            'ìˆëŠ”', 'ìˆì–´', 'ìˆë‹¤', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ë˜í•œ', 'ê·¸ëŸ°ë°',
            'ì´ëŠ”', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°ì„œ', 'ê±°ê¸°ì„œ', 'ì €ê¸°ì„œ', 'ë•Œë¬¸ì—',
            'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'í•˜ëŠ”', 'í•˜ê³ ', 'í•œë‹¤', 'ë©ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'ì…ë‹ˆë‹¤.',
            'ê²ƒì„', 'ê²ƒì´', 'ê²ƒì€', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'ì˜', 'ë¥¼', 'ì„',
            'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ê³¼', 'ì™€', 'ë„', 'ë§Œ',
            'ë¶€í„°', 'ê¹Œì§€', 'ìœ„í•´', 'ëŒ€í•´', 'ëŒ€í•œ', 'í†µí•´', 'ìœ„í•œ', 'ê°™ì€', 'ë‹¤ë¥¸'
        }
        
        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
        import re
        text = re.sub(r'[^\w\sê°€-í£]', ' ', text)
        words = text.split()
        
        # í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ
        keyword_candidates = []
        for word in words:
            clean_word = word.strip().lower()
            
            # ê¸¸ì´ ì œí•œ ë° ë¶ˆìš©ì–´ ì œê±°
            if (len(clean_word) > 2 and 
                clean_word not in stop_words and
                not clean_word.isdigit()):
                keyword_candidates.append(clean_word)
        
        # ë¹ˆë„ìˆ˜ ê³„ì‚°
        from collections import Counter
        word_freq = Counter(keyword_candidates)
        
        # ìƒìœ„ ë¹ˆë„ í‚¤ì›Œë“œ ì¶”ì¶œ (ìµœëŒ€ 10ê°œ)
        top_keywords = [word for word, freq in word_freq.most_common(10)]
        
        return top_keywords
    
    def _extract_topic(self, content: str) -> str:
        """ì£¼ì œ ì¶”ì¶œ - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜"""
        if not content:
            return "ì£¼ì œ ì¶”ì¶œ ì‹¤íŒ¨"
        
        lines = content.split('\n')
        
        # ì£¼ì œë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” íŒ¨í„´ë“¤
        topic_patterns = [
            r'ì£¼ì œ[:\s]*(.+)',
            r'topic[:\s]*(.+)',
            r'ì œëª©[:\s]*(.+)',
            r'title[:\s]*(.+)',
            r'í•µì‹¬[:\s]*(.+)',
            r'#\s*(.+)'  # ë§ˆí¬ë‹¤ìš´ ì œëª©
        ]
        
        for line in lines[:5]:  # ì²« 5ì¤„ì—ì„œ ì°¾ê¸°
            line = line.strip()
            for pattern in topic_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    topic = match.group(1).strip()
                    if len(topic) > 5 and len(topic) < 100:  # ì ì ˆí•œ ê¸¸ì´
                        return topic
        
        # íŒ¨í„´ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° ì²« ë²ˆì§¸ ë¬¸ì¥ ì‚¬ìš©
        sentences = content.split('.')
        for sentence in sentences[:3]:
            sentence = sentence.strip()
            if len(sentence) > 10 and len(sentence) < 100:
                return sentence + "."
        
        return "ì£¼ì œ ë¶„ì„ í•„ìš”"
    
    def _extract_summary(self, content: str) -> str:
        """ìš”ì•½ ì¶”ì¶œ - ê°œì„ ëœ ì•Œê³ ë¦¬ì¦˜"""
        if not content:
            return "ìš”ì•½ ì—†ìŒ"
        
        lines = content.split('\n')
        
        # ìš”ì•½ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” íŒ¨í„´ë“¤
        summary_patterns = [
            r'ìš”ì•½[:\s]*(.+)',
            r'summary[:\s]*(.+)', 
            r'í•µì‹¬[:\s]*(.+)',
            r'ê²°ë¡ [:\s]*(.+)',
            r'ê°œìš”[:\s]*(.+)'
        ]
        
        summary_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            for pattern in summary_patterns:
                match = re.search(pattern, line, re.IGNORECASE)
                if match:
                    summary_lines.append(match.group(1).strip())
        
        if summary_lines:
            summary = ' '.join(summary_lines)
            if len(summary) > 20:
                return summary[:200] + "..." if len(summary) > 200 else summary
        
        # íŒ¨í„´ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš° ë‚´ìš©ì˜ ì²˜ìŒ ë¶€ë¶„ ì‚¬ìš©
        sentences = content.split('.')
        key_sentences = []
        
        for sentence in sentences[:5]:
            sentence = sentence.strip()
            if len(sentence) > 20:  # ì˜ë¯¸ìˆëŠ” ë¬¸ì¥ ê¸¸ì´
                key_sentences.append(sentence)
                if len(' '.join(key_sentences)) > 150:
                    break
        
        if key_sentences:
            summary = '. '.join(key_sentences[:2]) + "."
            return summary[:200] + "..." if len(summary) > 200 else summary
        
        return "ìš”ì•½ ìƒì„± í•„ìš”"
